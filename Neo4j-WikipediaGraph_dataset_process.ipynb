{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pywikibot/tools/__init__.py:2023: UserWarning: File /Users/curtis/Desktop/Wikipedia-Graph-Topic-Suggestion/pywikibot.lwp had 644 mode; converted to 600 mode.\n",
      "  warn(warn_str.format(filename, st_mode - stat.S_IFREG, mode))\n"
     ]
    }
   ],
   "source": [
    "from py2neo import Graph\n",
    "from bs4 import BeautifulSoup\n",
    "import pywikibot\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import wikipedia\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import gensim\n",
    "from collections import defaultdict\n",
    "from time import sleep\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces, preprocess_string, remove_stopwords, strip_tags, strip_punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph(password=\"kshen3778\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'n': (_5214855:Category:Page {id: 690070, isNew: false, isRedirect: false, title: 'Futurama'})},\n",
       " {'n': (_5214856:Category:Page {id: 690451, isNew: false, isRedirect: false, title: 'World_War_II'})},\n",
       " {'n': (_5214857:Category:Page {id: 690571, isNew: false, isRedirect: false, title: 'Programming_languages'})},\n",
       " {'n': (_5214858:Category:Page {id: 690578, isNew: false, isRedirect: false, title: 'Professional_wrestling'})},\n",
       " {'n': (_5214859:Category:Page {id: 690637, isNew: false, isRedirect: false, title: 'Algebra'})},\n",
       " {'n': (_5214860:Category:Page {id: 690649, isNew: false, isRedirect: false, title: 'Anime'})},\n",
       " {'n': (_5214861:Category:Page {id: 690672, isNew: false, isRedirect: false, title: 'Abstract_algebra'})},\n",
       " {'n': (_5214862:Category:Page {id: 690747, isNew: false, isRedirect: false, title: 'Mathematics'})},\n",
       " {'n': (_5214863:Category:Page {id: 690777, isNew: false, isRedirect: false, title: 'Linear_algebra'})},\n",
       " {'n': (_5214864:Category:Page {id: 690803, isNew: false, isRedirect: false, title: 'Calculus'})},\n",
       " {'n': (_5214865:Category:Page {id: 690889, isNew: false, isRedirect: false, title: 'Monarchs'})},\n",
       " {'n': (_5214866:Category:Page {id: 690932, isNew: false, isRedirect: false, title: 'British_monarchs'})},\n",
       " {'n': (_5214867:Category:Page {id: 690990, isNew: false, isRedirect: false, title: 'Star_Trek'})},\n",
       " {'n': (_5214868:Category:Page {id: 691008, isNew: false, isRedirect: false, title: 'People'})},\n",
       " {'n': (_5214869:Category:Page {id: 691014, isNew: false, isRedirect: false, title: 'Popes'})},\n",
       " {'n': (_5214870:Category:Page {id: 691015, isNew: false, isRedirect: false, title: 'Desserts'})},\n",
       " {'n': (_5214871:Category:Page {id: 691023, isNew: false, isRedirect: false, title: 'Fruit'})},\n",
       " {'n': (_5214872:Category:Page {id: 691070, isNew: false, isRedirect: false, title: 'Lists'})},\n",
       " {'n': (_5214873:Category:Page {id: 691117, isNew: false, isRedirect: false, title: 'Computer_science'})},\n",
       " {'n': (_5214874:Category:Page {id: 691123, isNew: false, isRedirect: false, title: 'The_Simpsons'})},\n",
       " {'n': (_5214875:Category:Page {id: 691136, isNew: false, isRedirect: false, title: 'Algorithms'})},\n",
       " {'n': (_5214876:Category:Page {id: 691150, isNew: false, isRedirect: false, title: 'Data_structures'})},\n",
       " {'n': (_5214877:Category:Page {id: 691158, isNew: false, isRedirect: false, title: 'Monty_Python'})},\n",
       " {'n': (_5214878:Category:Page {id: 691160, isNew: false, isRedirect: false, title: 'Middle-earth_places'})},\n",
       " {'n': (_5214879:Category:Page {id: 691175, isNew: false, isRedirect: false, title: 'Middle-earth_characters'})}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.run(\"MATCH (n:Category) RETURN n LIMIT 25\").data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'n': (_5214918:Category:Page {id: 691614, isNew: false, isRedirect: false, title: 'Cold_War'})}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.run(\"MATCH (n:Category { title: 'Cold_War' }) RETURN n\").data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all sub cat\n",
    "cats = graph.run(\"MATCH (n:Category{title:'Constraint_logic_programming'})-[:BELONGS_TO*..1]-(p:Category) RETURN p LIMIT 100\").data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'p': (_6124108:Category:Page {id: 22968506, isNew: false, isRedirect: false, title: 'Constraint_programming'})},\n",
       " {'p': (_5594005:Category:Page {id: 1188828, isNew: false, isRedirect: false, title: 'Logic_programming'})}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Areas_of_computer_science',\n",
       " 'Cognitive_science',\n",
       " 'Computational_neuroscience',\n",
       " 'Cybernetics',\n",
       " 'Emerging_technologies',\n",
       " 'Formal_sciences',\n",
       " 'Futurology',\n",
       " 'Intelligence_by_type',\n",
       " 'Personhood',\n",
       " 'Technology_in_society',\n",
       " 'Unsolved_problems_in_computer_science']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getBadCategories(\"Artificial_intelligence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if a specific sub category references the select category an two way relationship (which we don't want)\n",
    "subs = graph.run(\"MATCH (n:Category{title:'Emerging_technologies'})-[:BELONGS_TO*..1]-(p:Category) RETURN p LIMIT 100\").data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify if category actually exists on online Wikipedia\n",
    "def categoryExist(name):\n",
    "    res = requests.get(\"https://en.wikipedia.org/wiki/Category:\" + name)\n",
    "    soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "    div = soup.find(\"div\", {\"id\": \"mw-normal-catlinks\"})\n",
    "    if(div):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#Use beautifulsoup to scrape the bottom categories of a parent category (these we don't want)\n",
    "def getBadCategories(parent):\n",
    "    res = requests.get(\"https://en.wikipedia.org/wiki/Category:\" + parent)\n",
    "    soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "    div = soup.find(\"div\", {\"id\": \"mw-normal-catlinks\"})\n",
    "    if(div):\n",
    "        to_return = []\n",
    "        for item in div.findAll(\"li\"):\n",
    "            to_return.append(\"_\".join(item.get_text().split(\" \")))\n",
    "        return to_return\n",
    "    else:\n",
    "        #parent does not exist\n",
    "        return []\n",
    "\n",
    "#get all immediate Wikipedia subcategories of main minus two way connections\n",
    "\n",
    "# def getAllSubCat(parent):\n",
    "#     all_cats = graph.run(\"MATCH (n:Category{title:'\" + parent + \"'})-[:BELONGS_TO*..1]-(p:Category) RETURN p LIMIT 100\").data()\n",
    "#     bad_cats = getBadCategories(parent)\n",
    "#     filtered = []\n",
    "#     for category in all_cats:\n",
    "#         cat_title = category['p'][\"title\"]\n",
    "#         if (not (cat_title in bad_cats)):\n",
    "#             filtered.append(cat_title)\n",
    "#     #search all these categories to see if any of them match the other way (if so, we remove)\n",
    "# #     filtered = []\n",
    "# #     for category in all_cats:\n",
    "# #         cat_title = category['p'][\"title\"]\n",
    "# #         subs = graph.run(\"MATCH (n:Category{title:'\" + cat_title + \"'})-[:BELONGS_TO*..1]-(p:Category) RETURN p LIMIT 100\").data()\n",
    "# #         found = False\n",
    "# #         for category_2 in subs:\n",
    "# #             sub_title = category_2['p'][\"title\"]\n",
    "# #             if(sub_title == parent):\n",
    "# #                 found = True\n",
    "# #         if (not found):\n",
    "# #             filtered.append(cat_title)\n",
    "    \n",
    "#     return filtered\n",
    "\n",
    "def getSubCategories(name):\n",
    "    site = pywikibot.Site()\n",
    "    gen = pywikibot.Category(site,'Category:' + name).subcategories(recurse=False)\n",
    "    subcats = []\n",
    "    for item in gen:\n",
    "        subcats.append(\"_\".join(item.aslink().split(\":\")[1][:-2].split(\" \")))\n",
    "    return subcats  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Borderline_personality_disorder_in_fiction',\n",
       " 'Histrionic_personality_disorder_in_fiction']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getAllSubCat(\"Works_about_personality\")\n",
    "#getBadCategories(\"Constraint_logic_programming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist = [\"July_events\"] #nodes that should not be visited\n",
    "all_nodes = []\n",
    "all_relations = [] #child parent\n",
    "visited = []\n",
    "def buildGraph(root, path):\n",
    "    path = path + \"/\" + root\n",
    "    print(\"Current Path: \" + path)\n",
    "    print()\n",
    "    if((not categoryExist(root)) or (root in blacklist)):\n",
    "        print(\"Category DNE or is BLACKLISTED\")\n",
    "        print()\n",
    "        return\n",
    "    if(root in visited):\n",
    "        print(\"VISITED ALREADY\")\n",
    "        print()\n",
    "        return\n",
    "    \n",
    "    visited.append(root)\n",
    "    \n",
    "    #add the node\n",
    "    all_nodes.append(root)\n",
    "    \n",
    "    #get all subcategories\n",
    "    sub_cat = getSubCategories(root)\n",
    "    if(len(sub_cat) == 0): #base case\n",
    "        return\n",
    "\n",
    "    \n",
    "    #add all the relationships\n",
    "    for item in sub_cat:\n",
    "        #check if this relationship already exists (cycle)\n",
    "        relation = item + \" \" + root\n",
    "        if(relation in all_relations):\n",
    "            print(\"Relation already exists: \", relation)\n",
    "            print()\n",
    "            return\n",
    "        all_relations.append(item + \" \" + root)\n",
    "   \n",
    "    #recurse on all sub nodes\n",
    "    for item in sub_cat:\n",
    "        buildGraph(item, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Path: /Machine_learning\n",
      "\n",
      "Current Path: /Machine_learning/Applied_machine_learning\n",
      "\n",
      "Current Path: /Machine_learning/Applied_machine_learning/AlphaGo\n",
      "\n",
      "Current Path: /Machine_learning/Artificial_neural_networks\n",
      "\n",
      "Current Path: /Machine_learning/Artificial_neural_networks/Deep_learning\n",
      "\n",
      "Current Path: /Machine_learning/Artificial_neural_networks/Neural_network_software\n",
      "\n",
      "Current Path: /Machine_learning/Bayesian_networks\n",
      "\n",
      "Current Path: /Machine_learning/Classification_algorithms\n",
      "\n",
      "Current Path: /Machine_learning/Classification_algorithms/Artificial_neural_networks\n",
      "\n",
      "VISITED ALREADY\n",
      "\n",
      "Current Path: /Machine_learning/Classification_algorithms/Decision_trees\n",
      "\n",
      "Current Path: /Machine_learning/Classification_algorithms/Ensemble_learning\n",
      "\n",
      "Current Path: /Machine_learning/Cluster_analysis\n",
      "\n",
      "Current Path: /Machine_learning/Cluster_analysis/Cluster_analysis_algorithms\n",
      "\n",
      "Current Path: /Machine_learning/Cluster_analysis/Clustering_criteria\n",
      "\n",
      "Current Path: /Machine_learning/Computational_learning_theory\n",
      "\n",
      "Current Path: /Machine_learning/Artificial_intelligence_conferences\n",
      "\n",
      "Current Path: /Machine_learning/Signal_processing_conferences\n",
      "\n",
      "Current Path: /Machine_learning/Data_mining_and_machine_learning_software\n",
      "\n",
      "Current Path: /Machine_learning/Data_mining_and_machine_learning_software/Social_network_analysis_software\n",
      "\n",
      "Current Path: /Machine_learning/Datasets_in_machine_learning\n",
      "\n",
      "Current Path: /Machine_learning/Datasets_in_machine_learning/Datasets_in_computer_vision\n",
      "\n",
      "Current Path: /Machine_learning/Deep_learning\n",
      "\n",
      "VISITED ALREADY\n",
      "\n",
      "Current Path: /Machine_learning/Dimension_reduction\n",
      "\n",
      "Current Path: /Machine_learning/Dimension_reduction/Factor_analysis\n",
      "\n",
      "Current Path: /Machine_learning/Ensemble_learning\n",
      "\n",
      "VISITED ALREADY\n",
      "\n",
      "Current Path: /Machine_learning/Evolutionary_algorithms\n",
      "\n",
      "Current Path: /Machine_learning/Evolutionary_algorithms/Gene_expression_programming\n",
      "\n",
      "Current Path: /Machine_learning/Evolutionary_algorithms/Genetic_algorithms\n",
      "\n",
      "Current Path: /Machine_learning/Evolutionary_algorithms/Genetic_algorithms/Artificial_immune_systems\n",
      "\n",
      "Current Path: /Machine_learning/Evolutionary_algorithms/Genetic_algorithms/Gene_expression_programming\n",
      "\n",
      "VISITED ALREADY\n",
      "\n",
      "Current Path: /Machine_learning/Evolutionary_algorithms/Genetic_programming\n",
      "\n",
      "Current Path: /Machine_learning/Evolutionary_algorithms/Nature-inspired_metaheuristics\n",
      "\n",
      "Current Path: /Machine_learning/Genetic_programming\n",
      "\n",
      "VISITED ALREADY\n",
      "\n",
      "Current Path: /Machine_learning/Inductive_logic_programming\n",
      "\n",
      "Current Path: /Machine_learning/Kernel_methods_for_machine_learning\n",
      "\n",
      "Current Path: /Machine_learning/Kernel_methods_for_machine_learning/Support_vector_machines\n",
      "\n",
      "Current Path: /Machine_learning/Latent_variable_models\n",
      "\n",
      "Current Path: /Machine_learning/Latent_variable_models/Factor_analysis\n",
      "\n",
      "VISITED ALREADY\n",
      "\n",
      "Current Path: /Machine_learning/Latent_variable_models/Structural_equation_models\n",
      "\n",
      "Current Path: /Machine_learning/Learning_in_computer_vision\n",
      "\n",
      "Current Path: /Machine_learning/Log-linear_models\n",
      "\n",
      "Current Path: /Machine_learning/Loss_functions\n",
      "\n",
      "Current Path: /Machine_learning/Machine_learning_algorithms\n",
      "\n",
      "Current Path: /Machine_learning/Machine_learning_algorithms/Genetic_algorithms\n",
      "\n",
      "VISITED ALREADY\n",
      "\n",
      "Current Path: /Machine_learning/Machine_learning_task\n",
      "\n",
      "Current Path: /Machine_learning/Markov_models\n",
      "\n",
      "Current Path: /Machine_learning/Markov_models/Hidden_Markov_models\n",
      "\n",
      "Current Path: /Machine_learning/Markov_models/Markov_networks\n",
      "\n",
      "Current Path: /Machine_learning/Ontology_learning_(computer_science)\n",
      "\n",
      "Current Path: /Machine_learning/Reinforcement_learning\n",
      "\n",
      "Current Path: /Machine_learning/Machine_learning_researchers\n",
      "\n",
      "Current Path: /Machine_learning/Semisupervised_learning\n",
      "\n",
      "Current Path: /Machine_learning/Statistical_natural_language_processing\n",
      "\n",
      "Current Path: /Machine_learning/Statistical_natural_language_processing/Language_modeling\n",
      "\n",
      "Current Path: /Machine_learning/Structured_prediction\n",
      "\n",
      "Current Path: /Machine_learning/Structured_prediction/Graphical_models\n",
      "\n",
      "Current Path: /Machine_learning/Structured_prediction/Graphical_models/Bayesian_networks\n",
      "\n",
      "VISITED ALREADY\n",
      "\n",
      "Current Path: /Machine_learning/Structured_prediction/Graphical_models/Causal_inference\n",
      "\n",
      "Current Path: /Machine_learning/Structured_prediction/Graphical_models/Markov_networks\n",
      "\n",
      "VISITED ALREADY\n",
      "\n",
      "Current Path: /Machine_learning/Supervised_learning\n",
      "\n",
      "Current Path: /Machine_learning/Support_vector_machines\n",
      "\n",
      "VISITED ALREADY\n",
      "\n",
      "Current Path: /Machine_learning/Unsupervised_learning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "buildGraph(\"Machine_learning\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine_learning',\n",
       " 'Applied_machine_learning',\n",
       " 'AlphaGo',\n",
       " 'Artificial_neural_networks',\n",
       " 'Deep_learning',\n",
       " 'Neural_network_software',\n",
       " 'Bayesian_networks',\n",
       " 'Classification_algorithms',\n",
       " 'Decision_trees',\n",
       " 'Ensemble_learning',\n",
       " 'Cluster_analysis',\n",
       " 'Cluster_analysis_algorithms',\n",
       " 'Clustering_criteria',\n",
       " 'Computational_learning_theory',\n",
       " 'Artificial_intelligence_conferences',\n",
       " 'Signal_processing_conferences',\n",
       " 'Data_mining_and_machine_learning_software',\n",
       " 'Social_network_analysis_software',\n",
       " 'Datasets_in_machine_learning',\n",
       " 'Datasets_in_computer_vision',\n",
       " 'Dimension_reduction',\n",
       " 'Factor_analysis',\n",
       " 'Evolutionary_algorithms',\n",
       " 'Gene_expression_programming',\n",
       " 'Genetic_algorithms',\n",
       " 'Artificial_immune_systems',\n",
       " 'Genetic_programming',\n",
       " 'Nature-inspired_metaheuristics',\n",
       " 'Inductive_logic_programming',\n",
       " 'Kernel_methods_for_machine_learning',\n",
       " 'Support_vector_machines',\n",
       " 'Latent_variable_models',\n",
       " 'Structural_equation_models',\n",
       " 'Learning_in_computer_vision',\n",
       " 'Log-linear_models',\n",
       " 'Loss_functions',\n",
       " 'Machine_learning_algorithms',\n",
       " 'Machine_learning_task',\n",
       " 'Markov_models',\n",
       " 'Hidden_Markov_models',\n",
       " 'Markov_networks',\n",
       " 'Ontology_learning_(computer_science)',\n",
       " 'Reinforcement_learning',\n",
       " 'Machine_learning_researchers',\n",
       " 'Semisupervised_learning',\n",
       " 'Statistical_natural_language_processing',\n",
       " 'Language_modeling',\n",
       " 'Structured_prediction',\n",
       " 'Graphical_models',\n",
       " 'Causal_inference',\n",
       " 'Supervised_learning',\n",
       " 'Unsupervised_learning']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In the domain of physics and probability, a Markov random field (often abbreviated as MRF), Markov network or undirected graphical model is a set of random variables having a Markov property described by an undirected graph. In other words, a random field is said to be a Markov random field if it satisfies Markov properties.\\nA Markov network or MRF is similar to a Bayesian network in its representation of dependencies; the differences being that Bayesian networks are directed and acyclic, whereas Markov networks are undirected and may be cyclic. Thus, a Markov network can represent certain dependencies that a Bayesian network cannot (such as cyclic dependencies); on the other hand, it can't represent certain dependencies that a Bayesian network can (such as induced dependencies). The underlying graph of a Markov random field may be finite or infinite.\\nWhen the joint probability density of the random variables is strictly positive, it is also referred to as a Gibbs random field, because, according to the Hammersley–Clifford theorem, it can then be represented by a Gibbs measure for an appropriate (locally defined) energy function. The prototypical Markov random field is the Ising model; indeed, the Markov random field was introduced as the general setting for the Ising model.\\nIn the domain of artificial intelligence, a Markov random field is used to model various low- to mid-level tasks in image processing and computer vision.\\n\\n\\n== Definition ==\\nGiven an undirected graph \\n  \\n    \\n      \\n        G\\n        =\\n        (\\n        V\\n        ,\\n        E\\n        )\\n      \\n    \\n    {\\\\displaystyle G=(V,E)}\\n  , a set of random variables \\n  \\n    \\n      \\n        X\\n        =\\n        (\\n        \\n          X\\n          \\n            v\\n          \\n        \\n        \\n          )\\n          \\n            v\\n            ∈\\n            V\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle X=(X_{v})_{v\\\\in V}}\\n   indexed by \\n  \\n    \\n      \\n        V\\n      \\n    \\n    {\\\\displaystyle V}\\n    form a Markov random field with respect to \\n  \\n    \\n      \\n        G\\n      \\n    \\n    {\\\\displaystyle G}\\n    if they satisfy the local Markov properties:\\n\\nPairwise Markov property: Any two non-adjacent variables are conditionally independent given all other variables:\\n  \\n    \\n      \\n        \\n          X\\n          \\n            u\\n          \\n        \\n        ⊥\\n        \\n        \\n        \\n        ⊥\\n        \\n          X\\n          \\n            v\\n          \\n        \\n        ∣\\n        \\n          X\\n          \\n            V\\n            ∖\\n            {\\n            u\\n            ,\\n            v\\n            }\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle X_{u}\\\\perp \\\\!\\\\!\\\\!\\\\perp X_{v}\\\\mid X_{V\\\\setminus \\\\{u,v\\\\}}}\\n  Local Markov property: A variable is conditionally independent of all other variables given its neighbors:\\n  \\n    \\n      \\n        \\n          X\\n          \\n            v\\n          \\n        \\n        ⊥\\n        \\n        \\n        \\n        ⊥\\n        \\n          X\\n          \\n            V\\n            ∖\\n            N\\n            \\u2061\\n            [\\n            v\\n            ]\\n          \\n        \\n        ∣\\n        \\n          X\\n          \\n            N\\n            \\u2061\\n            (\\n            v\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle X_{v}\\\\perp \\\\!\\\\!\\\\!\\\\perp X_{V\\\\setminus \\\\operatorname {N} [v]}\\\\mid X_{\\\\operatorname {N} (v)}}\\n  \\nwhere \\n  \\n    \\n      \\n        N\\n        \\u2061\\n        (\\n        v\\n        )\\n      \\n    \\n    {\\\\textstyle \\\\operatorname {N} (v)}\\n   is the set of neighbors of \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n  , and \\n  \\n    \\n      \\n        N\\n        \\u2061\\n        [\\n        v\\n        ]\\n        =\\n        v\\n        ∪\\n        N\\n        \\u2061\\n        (\\n        v\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\operatorname {N} [v]=v\\\\cup \\\\operatorname {N} (v)}\\n   is the closed neighbourhood of \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n  .Global Markov property: Any two subsets of variables are conditionally independent given a separating subset:\\n  \\n    \\n      \\n        \\n          X\\n          \\n            A\\n          \\n        \\n        ⊥\\n        \\n        \\n        \\n        ⊥\\n        \\n          X\\n          \\n            B\\n          \\n        \\n        ∣\\n        \\n          X\\n          \\n            S\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle X_{A}\\\\perp \\\\!\\\\!\\\\!\\\\perp X_{B}\\\\mid X_{S}}\\n  \\nwhere every path from a node in \\n  \\n    \\n      \\n        A\\n      \\n    \\n    {\\\\displaystyle A}\\n   to a node in \\n  \\n    \\n      \\n        B\\n      \\n    \\n    {\\\\displaystyle B}\\n   passes through \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n  .The Global Markov property is stronger than the Local Markov property, which in turn is stronger than the Pairwise one.  However, the above three Markov properties are equivalent for a positive probability.\\n\\n\\n== Clique factorization ==\\nAs the Markov property of an arbitrary probability distribution can be difficult to establish, a commonly used class of Markov random fields are those that can be factorized according to the cliques of the graph.\\nGiven a set of random variables \\n  \\n    \\n      \\n        X\\n        =\\n        (\\n        \\n          X\\n          \\n            v\\n          \\n        \\n        \\n          )\\n          \\n            v\\n            ∈\\n            V\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle X=(X_{v})_{v\\\\in V}}\\n  , let \\n  \\n    \\n      \\n        P\\n        (\\n        X\\n        =\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle P(X=x)}\\n   be the probability of a particular field configuration \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   in \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n  .  That is, \\n  \\n    \\n      \\n        P\\n        (\\n        X\\n        =\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle P(X=x)}\\n   is the probability of finding that the random variables \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n   take on the particular value \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  .  Because \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n   is a set, the probability of \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   should be understood to be taken with respect to a joint distribution of the \\n  \\n    \\n      \\n        \\n          X\\n          \\n            v\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle X_{v}}\\n  .\\nIf this joint density can be factorized over the cliques of \\n  \\n    \\n      \\n        G\\n      \\n    \\n    {\\\\displaystyle G}\\n  :\\n\\n  \\n    \\n      \\n        P\\n        (\\n        X\\n        =\\n        x\\n        )\\n        =\\n        \\n          ∏\\n          \\n            C\\n            ∈\\n            cl\\n            \\u2061\\n            (\\n            G\\n            )\\n          \\n        \\n        \\n          ϕ\\n          \\n            C\\n          \\n        \\n        (\\n        \\n          x\\n          \\n            C\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle P(X=x)=\\\\prod _{C\\\\in \\\\operatorname {cl} (G)}\\\\phi _{C}(x_{C})}\\n  then \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n   forms a Markov random field with respect to \\n  \\n    \\n      \\n        G\\n      \\n    \\n    {\\\\displaystyle G}\\n  .  Here, \\n  \\n    \\n      \\n        cl\\n        \\u2061\\n        (\\n        G\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\operatorname {cl} (G)}\\n   is the set of cliques of \\n  \\n    \\n      \\n        G\\n      \\n    \\n    {\\\\displaystyle G}\\n  .   The definition is equivalent if only maximal cliques are used. The functions φC are sometimes referred to as factor potentials or clique potentials. Note, however, conflicting terminology is in use: the word potential is often applied to the logarithm of φC.  This is because, in statistical mechanics, log(φC) has a direct interpretation as the potential energy of a configuration \\n  \\n    \\n      \\n        \\n          x\\n          \\n            C\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{C}}\\n  .\\nSome MRF's do not factorize: a simple example can be constructed on a cycle of 4 nodes with some infinite energies, i.e. configurations of zero probabilities, even if one, more appropriately, allows the infinite energies to act on the complete graph on \\n  \\n    \\n      \\n        V\\n      \\n    \\n    {\\\\displaystyle V}\\n  .MRF's factorize if at least one of the following conditions is fulfilled:\\n\\nthe density is positive (by the Hammersley–Clifford theorem)\\nthe graph is chordal (by equivalence to a Bayesian network)When such a factorization does exist, it is possible to construct a factor graph for the network.\\n\\n\\n== Exponential family ==\\nAny positive Markov random field can be written as exponential family in canonical form with feature functions \\n  \\n    \\n      \\n        \\n          f\\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f_{k}}\\n   such that the full-joint distribution can be written as\\n\\n  \\n    \\n      \\n        P\\n        (\\n        X\\n        =\\n        x\\n        )\\n        =\\n        \\n          \\n            1\\n            Z\\n          \\n        \\n        exp\\n        \\u2061\\n        \\n          (\\n          \\n            \\n              ∑\\n              \\n                k\\n              \\n            \\n            \\n              w\\n              \\n                k\\n              \\n              \\n                ⊤\\n              \\n            \\n            \\n              f\\n              \\n                k\\n              \\n            \\n            (\\n            \\n              x\\n              \\n                {\\n                k\\n                }\\n              \\n            \\n            )\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle P(X=x)={\\\\frac {1}{Z}}\\\\exp \\\\left(\\\\sum _{k}w_{k}^{\\\\top }f_{k}(x_{\\\\{k\\\\}})\\\\right)}\\n  where the notation\\n\\n  \\n    \\n      \\n        \\n          w\\n          \\n            k\\n          \\n          \\n            ⊤\\n          \\n        \\n        \\n          f\\n          \\n            k\\n          \\n        \\n        (\\n        \\n          x\\n          \\n            {\\n            k\\n            }\\n          \\n        \\n        )\\n        =\\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            \\n              N\\n              \\n                k\\n              \\n            \\n          \\n        \\n        \\n          w\\n          \\n            k\\n            ,\\n            i\\n          \\n        \\n        ⋅\\n        \\n          f\\n          \\n            k\\n            ,\\n            i\\n          \\n        \\n        (\\n        \\n          x\\n          \\n            {\\n            k\\n            }\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle w_{k}^{\\\\top }f_{k}(x_{\\\\{k\\\\}})=\\\\sum _{i=1}^{N_{k}}w_{k,i}\\\\cdot f_{k,i}(x_{\\\\{k\\\\}})}\\n  is simply a dot product over field configurations, and Z is the partition function:\\n\\n  \\n    \\n      \\n        Z\\n        =\\n        \\n          ∑\\n          \\n            x\\n            ∈\\n            \\n              \\n                X\\n              \\n            \\n          \\n        \\n        exp\\n        \\u2061\\n        \\n          (\\n          \\n            \\n              ∑\\n              \\n                k\\n              \\n            \\n            \\n              w\\n              \\n                k\\n              \\n              \\n                ⊤\\n              \\n            \\n            \\n              f\\n              \\n                k\\n              \\n            \\n            (\\n            \\n              x\\n              \\n                {\\n                k\\n                }\\n              \\n            \\n            )\\n          \\n          )\\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle Z=\\\\sum _{x\\\\in {\\\\mathcal {X}}}\\\\exp \\\\left(\\\\sum _{k}w_{k}^{\\\\top }f_{k}(x_{\\\\{k\\\\}})\\\\right).}\\n  Here, \\n  \\n    \\n      \\n        \\n          \\n            X\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {X}}}\\n   denotes the set of all possible assignments of values to all the network's random variables. Usually, the feature functions \\n  \\n    \\n      \\n        \\n          f\\n          \\n            k\\n            ,\\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f_{k,i}}\\n   are defined such that they are indicators of the clique's configuration, i.e. \\n  \\n    \\n      \\n        \\n          f\\n          \\n            k\\n            ,\\n            i\\n          \\n        \\n        (\\n        \\n          x\\n          \\n            {\\n            k\\n            }\\n          \\n        \\n        )\\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle f_{k,i}(x_{\\\\{k\\\\}})=1}\\n   if \\n  \\n    \\n      \\n        \\n          x\\n          \\n            {\\n            k\\n            }\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{\\\\{k\\\\}}}\\n   corresponds to the i-th possible configuration of the k-th clique and 0 otherwise. This model is equivalent to the clique factorization model given above, if \\n  \\n    \\n      \\n        \\n          N\\n          \\n            k\\n          \\n        \\n        =\\n        \\n          |\\n        \\n        dom\\n        \\u2061\\n        (\\n        \\n          C\\n          \\n            k\\n          \\n        \\n        )\\n        \\n          |\\n        \\n      \\n    \\n    {\\\\displaystyle N_{k}=|\\\\operatorname {dom} (C_{k})|}\\n   is the cardinality of the clique, and the weight of a feature \\n  \\n    \\n      \\n        \\n          f\\n          \\n            k\\n            ,\\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f_{k,i}}\\n   corresponds to the logarithm of the corresponding clique factor, i.e. \\n  \\n    \\n      \\n        \\n          w\\n          \\n            k\\n            ,\\n            i\\n          \\n        \\n        =\\n        log\\n        \\u2061\\n        ϕ\\n        (\\n        \\n          c\\n          \\n            k\\n            ,\\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle w_{k,i}=\\\\log \\\\phi (c_{k,i})}\\n  , where \\n  \\n    \\n      \\n        \\n          c\\n          \\n            k\\n            ,\\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle c_{k,i}}\\n   is the i-th possible configuration of the k-th clique, i.e. the i-th value in the domain of the clique \\n  \\n    \\n      \\n        \\n          C\\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle C_{k}}\\n  .\\nThe probability P is often called the Gibbs measure.  This expression of a Markov field as a logistic model is only possible if all clique factors are non-zero, i.e. if none of the elements of \\n  \\n    \\n      \\n        \\n          \\n            X\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {X}}}\\n   are assigned a probability of 0.  This allows  techniques from matrix algebra to be applied, e.g. that the trace of a matrix is log of the determinant, with the matrix representation of a graph arising from the graph's incidence matrix.\\nThe importance of the partition function Z is that many concepts from statistical mechanics, such as entropy, directly generalize to the case of Markov networks, and an intuitive understanding can thereby be gained.  In addition, the partition function allows variational methods to be applied to the solution of the problem: one can attach a driving force to one or more of the random variables, and explore the reaction of the network in response to this perturbation.  Thus, for example, one may add a driving term Jv, for each vertex v of the graph, to the partition function to get:\\n\\n  \\n    \\n      \\n        Z\\n        [\\n        J\\n        ]\\n        =\\n        \\n          ∑\\n          \\n            x\\n            ∈\\n            \\n              \\n                X\\n              \\n            \\n          \\n        \\n        exp\\n        \\u2061\\n        \\n          (\\n          \\n            \\n              ∑\\n              \\n                k\\n              \\n            \\n            \\n              w\\n              \\n                k\\n              \\n              \\n                ⊤\\n              \\n            \\n            \\n              f\\n              \\n                k\\n              \\n            \\n            (\\n            \\n              x\\n              \\n                {\\n                k\\n                }\\n              \\n            \\n            )\\n            +\\n            \\n              ∑\\n              \\n                v\\n              \\n            \\n            \\n              J\\n              \\n                v\\n              \\n            \\n            \\n              x\\n              \\n                v\\n              \\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle Z[J]=\\\\sum _{x\\\\in {\\\\mathcal {X}}}\\\\exp \\\\left(\\\\sum _{k}w_{k}^{\\\\top }f_{k}(x_{\\\\{k\\\\}})+\\\\sum _{v}J_{v}x_{v}\\\\right)}\\n  Formally differentiating with respect to Jv gives the expectation value of the random variable Xv associated with the vertex v:\\n\\n  \\n    \\n      \\n        E\\n        [\\n        \\n          X\\n          \\n            v\\n          \\n        \\n        ]\\n        =\\n        \\n          \\n            1\\n            Z\\n          \\n        \\n        \\n          \\n            \\n            \\n              \\n                \\n                  ∂\\n                  Z\\n                  [\\n                  J\\n                  ]\\n                \\n                \\n                  ∂\\n                  \\n                    J\\n                    \\n                      v\\n                    \\n                  \\n                \\n              \\n            \\n            |\\n          \\n          \\n            \\n              J\\n              \\n                v\\n              \\n            \\n            =\\n            0\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle E[X_{v}]={\\\\frac {1}{Z}}\\\\left.{\\\\frac {\\\\partial Z[J]}{\\\\partial J_{v}}}\\\\right|_{J_{v}=0}.}\\n  Correlation functions are computed likewise; the two-point correlation is:\\n\\n  \\n    \\n      \\n        C\\n        [\\n        \\n          X\\n          \\n            u\\n          \\n        \\n        ,\\n        \\n          X\\n          \\n            v\\n          \\n        \\n        ]\\n        =\\n        \\n          \\n            1\\n            Z\\n          \\n        \\n        \\n          \\n            \\n            \\n              \\n                \\n                  \\n                    ∂\\n                    \\n                      2\\n                    \\n                  \\n                  Z\\n                  [\\n                  J\\n                  ]\\n                \\n                \\n                  ∂\\n                  \\n                    J\\n                    \\n                      u\\n                    \\n                  \\n                  ∂\\n                  \\n                    J\\n                    \\n                      v\\n                    \\n                  \\n                \\n              \\n            \\n            |\\n          \\n          \\n            \\n              J\\n              \\n                u\\n              \\n            \\n            =\\n            0\\n            ,\\n            \\n              J\\n              \\n                v\\n              \\n            \\n            =\\n            0\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle C[X_{u},X_{v}]={\\\\frac {1}{Z}}\\\\left.{\\\\frac {\\\\partial ^{2}Z[J]}{\\\\partial J_{u}\\\\partial J_{v}}}\\\\right|_{J_{u}=0,J_{v}=0}.}\\n  Unfortunately, though the likelihood of a logistic Markov network is convex, evaluating the likelihood or gradient of the likelihood of a model requires inference in the model, which is generally computationally infeasible (see 'Inference' below).\\n\\n\\n== Examples ==\\n\\n\\n=== Gaussian ===\\nA multivariate normal distribution forms a Markov random field with respect to a graph \\n  \\n    \\n      \\n        G\\n        =\\n        (\\n        V\\n        ,\\n        E\\n        )\\n      \\n    \\n    {\\\\displaystyle G=(V,E)}\\n   if the missing edges correspond to zeros on the precision matrix (the inverse covariance matrix):\\n\\n  \\n    \\n      \\n        X\\n        =\\n        (\\n        \\n          X\\n          \\n            v\\n          \\n        \\n        \\n          )\\n          \\n            v\\n            ∈\\n            V\\n          \\n        \\n        ∼\\n        \\n          \\n            N\\n          \\n        \\n        (\\n        \\n          μ\\n        \\n        ,\\n        Σ\\n        )\\n      \\n    \\n    {\\\\displaystyle X=(X_{v})_{v\\\\in V}\\\\sim {\\\\mathcal {N}}({\\\\boldsymbol {\\\\mu }},\\\\Sigma )}\\n  such that\\n\\n  \\n    \\n      \\n        (\\n        \\n          Σ\\n          \\n            −\\n            1\\n          \\n        \\n        \\n          )\\n          \\n            u\\n            v\\n          \\n        \\n        =\\n        0\\n        \\n        \\n          iff\\n        \\n        \\n        {\\n        u\\n        ,\\n        v\\n        }\\n        ∉\\n        E\\n        .\\n      \\n    \\n    {\\\\displaystyle (\\\\Sigma ^{-1})_{uv}=0\\\\quad {\\\\text{iff}}\\\\quad \\\\{u,v\\\\}\\\\notin E.}\\n  \\n\\n\\n== Inference ==\\nAs in a Bayesian network, one may calculate the conditional distribution of a set of nodes \\n  \\n    \\n      \\n        \\n          V\\n          ′\\n        \\n        =\\n        {\\n        \\n          v\\n          \\n            1\\n          \\n        \\n        ,\\n        …\\n        ,\\n        \\n          v\\n          \\n            i\\n          \\n        \\n        }\\n      \\n    \\n    {\\\\displaystyle V'=\\\\{v_{1},\\\\ldots ,v_{i}\\\\}}\\n   given values to another set of nodes \\n  \\n    \\n      \\n        \\n          W\\n          ′\\n        \\n        =\\n        {\\n        \\n          w\\n          \\n            1\\n          \\n        \\n        ,\\n        …\\n        ,\\n        \\n          w\\n          \\n            j\\n          \\n        \\n        }\\n      \\n    \\n    {\\\\displaystyle W'=\\\\{w_{1},\\\\ldots ,w_{j}\\\\}}\\n   in the Markov random field by summing over all possible assignments to \\n  \\n    \\n      \\n        u\\n        ∉\\n        \\n          V\\n          ′\\n        \\n        ,\\n        \\n          W\\n          ′\\n        \\n      \\n    \\n    {\\\\displaystyle u\\\\notin V',W'}\\n  ; this is called exact inference. However, exact inference is a #P-complete problem, and thus computationally intractable in the general case. Approximation techniques such as Markov chain Monte Carlo and loopy belief propagation are often more feasible in practice.  Some particular subclasses of MRFs, such as trees (see Chow–Liu tree), have polynomial-time inference algorithms; discovering such subclasses is an active research topic.  There are also subclasses of MRFs that permit efficient MAP, or most likely assignment, inference; examples of these include associative networks. Another interesting sub-class is the one of decomposable models (when the graph is chordal): having a closed-form for the MLE, it is possible to discover a consistent structure for hundreds of variables.\\n\\n\\n== Conditional random fields ==\\n\\nOne notable variant of a Markov random field is a conditional random field, in which each random variable may also be conditioned upon a set of global observations \\n  \\n    \\n      \\n        o\\n      \\n    \\n    {\\\\displaystyle o}\\n  . In this model, each function \\n  \\n    \\n      \\n        \\n          ϕ\\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\phi _{k}}\\n   is a mapping from all assignments to both the clique k and the observations \\n  \\n    \\n      \\n        o\\n      \\n    \\n    {\\\\displaystyle o}\\n   to the nonnegative real numbers. This form of the Markov network may be more appropriate for producing discriminative classifiers, which do not model the distribution over the observations. CRFs were proposed by John D. Lafferty, Andrew McCallum and Fernando C.N. Pereira in 2001.\\n\\n\\n== Varied applications ==\\nMarkov random fields find application in a variety of fields, ranging from computer graphics to computer vision, machine learning or computational biology. MRFs are used in image processing to generate textures as they can be used to generate flexible and stochastic image models. In image modelling, the task is to find a suitable intensity distribution of a given image, where suitability depends on the kind of task and MRFs are flexible enough to be used for image and texture synthesis, image compression and restoration, image segmentation, 3D image inference from 2D images, image registration, texture synthesis, super-resolution, stereo matching and information retrieval. They can be used to solve various computer vision problems which can be posed as energy minimization problems or problems where different regions have to be distinguished using a set of discriminating features, within a Markov random field framework, to predict the category of the region.  Markov random fields were a generalization over the  Ising model and have, since then, been used widely in combinatorial optimizations and networks.\\n\\n\\n== See also ==\\nConstraint Composite Graph\\nGraphical model\\nHammersley–Clifford theorem\\nHopfield network\\nInteracting particle system\\nIsing model\\nLog-linear analysis\\nMarkov chain\\nMarkov logic network\\nMaximum entropy method\\nProbabilistic cellular automata\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nMRF implementation in C++ for regular 2D lattices\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site = pywikibot.Site('en', 'wikipedia')  # The site we want to run our bot on\n",
    "#page = pywikibot.Page(site, 'Support_vector_machines')\n",
    "#page.text\n",
    "\n",
    "page = wikipedia.page(\"Markov_networks\")\n",
    "page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save graph to files\n",
    "fileName = \"data/ML\"\n",
    "with open(fileName + '_nodes.txt', 'w') as f:\n",
    "    for item in all_nodes:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(fileName + '_relations.txt', 'w') as f:\n",
    "    for item in all_relations:\n",
    "        f.write(\"%s\\n\" % item)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    x = gensim.utils.simple_preprocess(x)\n",
    "    x = \" \".join(x)\n",
    "    \n",
    "    CUSTOM_FILTERS = [lambda x: x.lower(), strip_tags, strip_punctuation, remove_stopwords]\n",
    "    x = preprocess_string(x, CUSTOM_FILTERS)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==Machine_learning\n",
      "==\n",
      "==Applied_machine_learning\n",
      "==\n",
      "==AlphaGo\n",
      "==\n",
      "==Artificial_neural_networks\n",
      "==\n",
      "==Deep_learning\n",
      "==\n",
      "==Neural_network_software\n",
      "==\n",
      "==Bayesian_networks\n",
      "==\n",
      "==Classification_algorithms\n",
      "==\n",
      "==Decision_trees\n",
      "==\n",
      "==Ensemble_learning\n",
      "==\n",
      "==Cluster_analysis\n",
      "==\n",
      "==Cluster_analysis_algorithms\n",
      "==\n",
      "==Clustering_criteria\n",
      "==\n",
      "==Computational_learning_theory\n",
      "==\n",
      "==Artificial_intelligence_conferences\n",
      "==\n",
      "==Signal_processing_conferences\n",
      "==\n",
      "==Data_mining_and_machine_learning_software\n",
      "==\n",
      "==Social_network_analysis_software\n",
      "==\n",
      "==Datasets_in_machine_learning\n",
      "==\n",
      "==Datasets_in_computer_vision\n",
      "==\n",
      "==Dimension_reduction\n",
      "==\n",
      "==Factor_analysis\n",
      "==\n",
      "==Evolutionary_algorithms\n",
      "==\n",
      "==Gene_expression_programming\n",
      "==\n",
      "==Genetic_algorithms\n",
      "==\n",
      "==Artificial_immune_systems\n",
      "==\n",
      "==Genetic_programming\n",
      "==\n",
      "==Nature-inspired_metaheuristics\n",
      "==\n",
      "==Inductive_logic_programming\n",
      "==\n",
      "==Kernel_methods_for_machine_learning\n",
      "==\n",
      "==Support_vector_machines\n",
      "==\n",
      "==Latent_variable_models\n",
      "==\n",
      "==Structural_equation_models\n",
      "==\n",
      "==Learning_in_computer_vision\n",
      "==\n",
      "==Log-linear_models\n",
      "==\n",
      "==Loss_functions\n",
      "==\n",
      "==Machine_learning_algorithms\n",
      "==\n",
      "==Machine_learning_task\n",
      "==\n",
      "==Markov_models\n",
      "==\n",
      "==Hidden_Markov_models\n",
      "==\n",
      "==Markov_networks\n",
      "==\n",
      "==Ontology_learning_(computer_science)\n",
      "==\n",
      "==Reinforcement_learning\n",
      "==\n",
      "==Machine_learning_researchers\n",
      "==\n",
      "==Semisupervised_learning\n",
      "==\n",
      "==Statistical_natural_language_processing\n",
      "==\n",
      "==Language_modeling\n",
      "==\n",
      "==Structured_prediction\n",
      "==\n",
      "==Graphical_models\n",
      "==\n",
      "==Causal_inference\n",
      "==\n",
      "==Supervised_learning\n",
      "==\n",
      "==Unsupervised_learning\n",
      "==\n"
     ]
    }
   ],
   "source": [
    "#Extract wikipedia articles (to convert into feature vectors)\n",
    "file = open(fileName + '_nodes.txt', \"r\")\n",
    "articles_cleaned = []\n",
    "freq_threshold = 3\n",
    "for line in file:\n",
    "    #sleep(10)\n",
    "    print(\"==\" + line + \"==\")\n",
    "#     all_good = True\n",
    "#     while(True):\n",
    "    try:\n",
    "        search = wikipedia.search(line.rstrip(\"\\n\"))[0]\n",
    "        page = wikipedia.page(search)\n",
    "    except wikipedia.DisambiguationError as e:\n",
    "        page = wikipedia.page(e.options[0])\n",
    "    except wikipedia.WikipediaException as e:\n",
    "        all_good = False\n",
    "    except Exception:\n",
    "        articles_cleaned.append(\"\")\n",
    "        continue;\n",
    "#         if all_good:\n",
    "#             break;\n",
    "    #page = wikipedia.page(wikipedia.search(line.rstrip(\"\\n\"))[0])\n",
    "    content = page.content\n",
    "    cleaned = preprocess(content) #strip punctuations and stopwords and weird characters\n",
    "    cleaned = \" \".join(cleaned)\n",
    "    doc = nlp(cleaned)\n",
    "    lemm = [token.lemma_ for token in doc]\n",
    "    d = defaultdict(int)\n",
    "    for item in lemm:\n",
    "        d[item] += 1\n",
    "    tokens=[key for key,value in d.items() if value>freq_threshold] #all words with frequency of more than some number\n",
    "    texts = [word for word in lemm if word in tokens]\n",
    "    texts = \" \".join(texts)\n",
    "    articles_cleaned.append(texts)\n",
    "    #print(texts)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaai', 'ab', 'ability', 'able', 'abstract', 'access', 'account', 'accuracy', 'achieve', 'acm', 'acquire', 'acquisition', 'act', 'action', 'activation', 'actual', 'acyclic', 'adaptation', 'adaptive', 'add', 'addition', 'address', 'adjust', 'advertising', 'agent', 'ai', 'aid', 'aim', 'ais', 'al', 'algorithm', 'algorithmic', 'align', 'allow', 'alpha', 'alphago', 'alphazero', 'analysis', 'analyze', 'ann', 'anneal', 'annotation', 'anns', 'annual', 'anomaly', 'answer', 'ant', 'anti', 'antibody', 'ap', 'application', 'apply', 'approach', 'appropriate', 'approx', 'approximate', 'approximation', 'aptitude', 'arbitrary', 'architecture', 'area', 'argmax', 'argmin', 'argue', 'argument', 'arise', 'arity', 'array', 'arrive', 'arrow', 'art', 'artificial', 'asr', 'assess', 'assign', 'assignment', 'associate', 'association', 'assume', 'assumption', 'assure', 'attempt', 'attribute', 'auditory', 'author', 'automate', 'automatic', 'automatically', 'autonomous', 'available', 'average', 'averaging', 'award', 'axis', 'background', 'backpropagation', 'bag', 'bagging', 'base', 'basic', 'batch', 'baye', 'bayesian', 'beach', 'beat', 'begin', 'behavior', 'belief', 'bellman', 'belong', 'benchmark', 'beta', 'better', 'bias', 'big', 'binary', 'biological', 'bit', 'blanket', 'block', 'blood', 'bma', 'bmc', 'board', 'boldsymbol', 'book', 'boolean', 'boost', 'border', 'bouldin', 'bound', 'boundary', 'brain', 'branch', 'broad', 'bucket', 'build', 'building', 'caa', 'call', 'camera', 'cancer', 'candidate', 'canonical', 'cap', 'capable', 'capacity', 'capitalize', 'car', 'care', 'carlo', 'case', 'cat', 'category', 'cattell', 'causal', 'causality', 'causation', 'cause', 'cdot', 'cell', 'cellular', 'center', 'centroid', 'certain', 'chain', 'challenge', 'champion', 'chance', 'change', 'characteristic', 'chess', 'child', 'chinese', 'choice', 'choose', 'chromosome', 'chunk', 'citeseer', 'class', 'classification', 'classifier', 'classify', 'clause', 'click', 'climb', 'clonal', 'close', 'closely', 'cluster', 'clustering', 'cnn', 'cnns', 'code', 'coefficient', 'collection', 'colony', 'color', 'column', 'combination', 'combinatorial', 'combine', 'come', 'commercial', 'common', 'commonly', 'communalitie', 'communality', 'community', 'compare', 'competitive', 'complete', 'complex', 'complexity', 'component', 'compose', 'computation', 'computational', 'compute', 'computer', 'computing', 'concept', 'concern', 'condition', 'conditional', 'conference', 'confusion', 'connect', 'connection', 'connectivity', 'consequence', 'consider', 'consist', 'consistency', 'constant', 'constraint', 'construct', 'contain', 'content', 'contest', 'context', 'continuous', 'contrast', 'control', 'convergence', 'convex', 'convolutional', 'coordinate', 'copy', 'correct', 'correlate', 'correlation', 'correspond', 'cost', 'count', 'cov', 'covariance', 'cover', 'create', 'cresceptron', 'criterion', 'cross', 'crossbar', 'crossover', 'current', 'custom', 'dag', 'dan', 'dasgupta', 'data', 'database', 'dataset', 'datum', 'dau', 'day', 'dbscan', 'dc', 'dca', 'deal', 'december', 'decision', 'deep', 'deepmind', 'defeat', 'defense', 'define', 'definition', 'degree', 'delta', 'demonstrate', 'density', 'depend', 'dependency', 'dependent', 'depth', 'derive', 'descendant', 'descent', 'description', 'design', 'desire', 'detect', 'detection', 'determine', 'develop', 'development', 'diagonal', 'diagram', 'dictionary', 'differ', 'difference', 'different', 'difficult', 'digital', 'dimension', 'dimensional', 'dimensionality', 'direct', 'discipline', 'discourse', 'discover', 'discovery', 'discriminant', 'disease', 'display', 'displaystyle', 'distance', 'distinct', 'distribution', 'diversity', 'divide', 'dnn', 'dnns', 'document', 'dog', 'doi', 'domain', 'door', 'dot', 'draw', 'drop', 'drug', 'dt', 'dual', 'dunn', 'dw', 'dynamic', 'early', 'easily', 'economic', 'edge', 'edu', 'efa', 'effect', 'efficiency', 'efficient', 'efficiently', 'eigenvalue', 'eigenvector', 'elbow', 'element', 'ell', 'email', 'embed', 'emotion', 'empirical', 'employ', 'encode', 'end', 'engineer', 'engineering', 'english', 'ensemble', 'entailment', 'entity', 'environment', 'epidemiology', 'epsilon', 'equal', 'equation', 'error', 'especially', 'estimate', 'estimation', 'et', 'evaluate', 'evaluation', 'event', 'evidence', 'evolution', 'evolutionary', 'evolve', 'exact', 'example', 'exchange', 'exist', 'expect', 'experience', 'expert', 'explain', 'explanatory', 'exploitation', 'exploration', 'exploratory', 'explore', 'express', 'expression', 'extend', 'extension', 'external', 'extra', 'extract', 'extraction', 'fa', 'fabrigar', 'face', 'facebook', 'fact', 'factor', 'factoring', 'fail', 'false', 'family', 'fan', 'feature', 'feedforward', 'fem', 'field', 'filter', 'financial', 'find', 'finite', 'fit', 'fitness', 'fix', 'flow', 'fn', 'focus', 'follow', 'forecasting', 'foreground', 'forest', 'form', 'formula', 'fourth', 'fowlke', 'fp', 'frac', 'frame', 'framework', 'fraud', 'free', 'frequentist', 'frequently', 'function', 'future', 'ga', 'gain', 'game', 'gamma', 'gas', 'gather', 'gaussian', 'gda', 'gene', 'general', 'generality', 'generalization', 'generally', 'generate', 'generation', 'generative', 'genetic', 'genexprotool', 'genome', 'genotype', 'geometry', 'gep', 'geq', 'give', 'global', 'glover', 'goal', 'good', 'google', 'gp', 'gpus', 'gradient', 'gram', 'grammar', 'graph', 'graphic', 'graphical', 'graphs', 'grass', 'ground', 'group', 'growth', 'guarantee', 'guidance', 'guide', 'hand', 'handicap', 'handle', 'handwritten', 'hard', 'hardware', 'hat', 'haussler', 'head', 'health', 'hebbian', 'help', 'heuristic', 'hide', 'hierarchical', 'hierarchy', 'high', 'hill', 'hinge', 'histogram', 'hit', 'holland', 'homeotic', 'house', 'ht', 'html', 'http', 'huang', 'hui', 'human', 'hyperparameter', 'hyperplane', 'hypothesis', 'identification', 'identify', 'ilp', 'image', 'imagenet', 'immune', 'immunology', 'implement', 'implementation', 'imply', 'important', 'improve', 'include', 'increase', 'independence', 'independent', 'index', 'indexthe', 'indicate', 'individual', 'induce', 'induction', 'inductive', 'industrial', 'industry', 'infer', 'inference', 'influence', 'information', 'initial', 'initially', 'inner', 'input', 'insertion', 'inspection', 'inspire', 'instance', 'instead', 'integer', 'integrate', 'intelligence', 'inter', 'interaction', 'interactive', 'interface', 'internal', 'interpret', 'interpretation', 'intervention', 'intra', 'introduce', 'introduction', 'inverse', 'inversion', 'involve', 'isbn', 'issue', 'ist', 'item', 'iteration', 'jaccard', 'january', 'java', 'jie', 'john', 'joint', 'journal', 'ke', 'kearn', 'kernel', 'kind', 'knapsack', 'know', 'knowledge', 'korea', 'koza', 'label', 'labelme', 'lack', 'lambda', 'land', 'landscape', 'langle', 'language', 'laplacian', 'large', 'latent', 'later', 'law', 'layer', 'lda', 'ldot', 'lead', 'learn', 'learning', 'leave', 'lee', 'leftarrow', 'leiman', 'length', 'leq', 'let', 'level', 'lie', 'lifeguard', 'light', 'like', 'likelihood', 'line', 'linear', 'linearly', 'link', 'linkage', 'list', 'literal', 'ln', 'lnot', 'load', 'loading', 'local', 'log', 'logic', 'logistic', 'long', 'look', 'lor', 'lose', 'loss', 'low', 'lstm', 'lvert', 'machine', 'main', 'maintain', 'major', 'make', 'mallow', 'malware', 'manifold', 'manipulation', 'map', 'margin', 'mark', 'market', 'markov', 'master', 'match', 'mathbb', 'mathbf', 'mathcal', 'mathematical', 'mathrm', 'matrix', 'max', 'maximize', 'maximum', 'mdp', 'mdps', 'mean', 'measure', 'measurement', 'medical', 'member', 'memetic', 'memory', 'mercer', 'meta', 'metaheuristic', 'method', 'methodology', 'mid', 'military', 'million', 'minimization', 'minimize', 'mining', 'missile', 'mixture', 'mobile', 'model', 'modification', 'molecular', 'moment', 'monte', 'morphology', 'motion', 'move', 'mu', 'multi', 'multicellular', 'multiclass', 'multigenic', 'multiple', 'mutate', 'mutation', 'mutual', 'na', 'naive', 'name', 'native', 'natural', 'nature', 'near', 'nearest', 'necessarily', 'necessity', 'need', 'neg', 'negated', 'negative', 'neighbor', 'net', 'network', 'neural', 'neuron', 'new', 'nicosia', 'nlp', 'nmax', 'nmf', 'nn', 'node', 'noise', 'noisy', 'nominal', 'non', 'nonlinear', 'norm', 'normal', 'note', 'noun', 'number', 'numeral', 'numeric', 'numerical', 'object', 'objective', 'oblique', 'observable', 'observation', 'observe', 'observed', 'obtain', 'october', 'official', 'offspring', 'old', 'ontology', 'open', 'operation', 'operator', 'operatorname', 'optic', 'optical', 'optimal', 'optimality', 'optimally', 'optimization', 'optimize', 'optimum', 'order', 'ordering', 'organism', 'organize', 'original', 'orthogonal', 'outcome', 'outlier', 'outline', 'output', 'overfitte', 'overlap', 'pa', 'pac', 'package', 'page', 'pair', 'panel', 'par', 'parallel', 'paramet', 'parameter', 'parent', 'parse', 'partially', 'particle', 'particular', 'particularly', 'partition', 'partitioning', 'path', 'pattern', 'pc', 'pca', 'pdf', 'penalty', 'people', 'perception', 'perform', 'performance', 'perp', 'person', 'personality', 'phenomenon', 'phenotype', 'physics', 'pi', 'platform', 'play', 'player', 'pm', 'pmml', 'point', 'policy', 'polygon', 'polynomial', 'pool', 'popular', 'population', 'pose', 'position', 'positive', 'possible', 'posterior', 'potential', 'potentially', 'power', 'powerful', 'pp', 'pr', 'pre', 'precision', 'predict', 'prediction', 'predictive', 'preprocesse', 'preserve', 'press', 'prevent', 'previous', 'previously', 'primal', 'principal', 'principle', 'prior', 'probabilistic', 'probability', 'probably', 'problem', 'procedure', 'proceeding', 'process', 'processing', 'produce', 'product', 'professional', 'progol', 'program', 'programming', 'progress', 'project', 'projection', 'promote', 'property', 'propose', 'prove', 'provide', 'psu', 'psychology', 'psychometric', 'publication', 'publish', 'purity', 'purpose', 'quadratic', 'quality', 'quantitative', 'quantity', 'query', 'question', 'rain', 'rand', 'random', 'randomly', 'range', 'rangle', 'rank', 'rate', 'raw', 'reach', 'real', 'reason', 'recall', 'receive', 'recent', 'recognition', 'recognize', 'recombination', 'recombine', 'recommendation', 'reconstruction', 'record', 'recurrent', 'red', 'reduce', 'reduction', 'refer', 'region', 'regression', 'regret', 'regularization', 'regularize', 'reinforcement', 'relate', 'related', 'relation', 'relationship', 'relative', 'relevant', 'rely', 'removal', 'replace', 'replication', 'report', 'represent', 'representation', 'reproduction', 'require', 'research', 'researcher', 'residual', 'resignation', 'resource', 'respect', 'result', 'retain', 'return', 'reward', 'rho', 'right', 'rightarrow', 'rise', 'risk', 'rnc', 'rnn', 'robot', 'root', 'rotation', 'roulette', 'rule', 'run', 'rvert', 'sample', 'sampler', 'say', 'scale', 'scanner', 'scene', 'schemata', 'scheme', 'schmid', 'science', 'scientific', 'score', 'scree', 'search', 'second', 'sedol', 'see', 'seek', 'segmentation', 'select', 'selection', 'self', 'semantic', 'semi', 'sense', 'sensor', 'sentence', 'separate', 'separation', 'sequence', 'serve', 'server', 'set', 'sgn', 'shallow', 'shape', 'shift', 'short', 'show', 'sigma', 'sign', 'signal', 'significant', 'silhouette', 'sim', 'similar', 'similarity', 'simple', 'simplicity', 'simulate', 'simulation', 'simulator', 'single', 'situation', 'size', 'small', 'smooth', 'smoothness', 'social', 'soft', 'software', 'solution', 'solve', 'sound', 'south', 'space', 'sparse', 'speak', 'speaker', 'special', 'specific', 'specifically', 'specify', 'speech', 'speed', 'springer', 'sprinkler', 'sps', 'square', 'squared', 'standard', 'standardize', 'star', 'start', 'state', 'stationary', 'statistic', 'statistical', 'step', 'stochastic', 'stone', 'strategy', 'strength', 'string', 'strong', 'structural', 'structure', 'student', 'study', 'style', 'sub', 'subject', 'subjective', 'subset', 'subspace', 'subtask', 'success', 'successfully', 'successor', 'sufficiency', 'suggest', 'sum', 'summit', 'supervise', 'supervised', 'support', 'suppose', 'sutton', 'svm', 'svms', 'swarm', 'symbol', 'symbolic', 'symmetric', 'system', 'table', 'tabu', 'tail', 'take', 'tame', 'target', 'task', 'taxonomic', 'td', 'team', 'technique', 'technology', 'temporal', 'tend', 'term', 'terminal', 'test', 'text', 'textit', 'textstyle', 'textual', 'th', 'theorem', 'theoretical', 'theory', 'theta', 'think', 'threshold', 'tie', 'time', 'timit', 'title', 'tlearn', 'tn', 'tool', 'toolkit', 'topic', 'total', 'tp', 'tracking', 'tractable', 'tradeoff', 'traditional', 'trail', 'train', 'training', 'transductive', 'transform', 'transition', 'translate', 'translation', 'transposition', 'traverse', 'tree', 'trick', 'true', 'truth', 'tsvm', 'turn', 'tutorial', 'type', 'typical', 'typically', 'uncertainty', 'uncorrelated', 'underlie', 'underset', 'understand', 'understanding', 'undirected', 'unigram', 'unique', 'unit', 'unity', 'universal', 'university', 'unknown', 'unlabele', 'unlabeled', 'unobserved', 'unsupervised', 'use', 'user', 'usual', 'usually', 'utility', 'valid', 'validation', 'value', 'vapnik', 'varepsilon', 'variable', 'variance', 'variant', 'variation', 'variety', 'varimax', 'varphi', 'vary', 'vdot', 'vec', 'vector', 'vehicle', 'verbal', 'verification', 'version', 'versus', 'victory', 'video', 'view', 'visible', 'vision', 'visual', 'visualization', 'vladimir', 'vocabulary', 'vol', 'vote', 'waikato', 'warmuth', 'way', 'weak', 'weight', 'weka', 'well', 'wet', 'wheel', 'win', 'wintempla', 'word', 'wordnet', 'work', 'world', 'write', 'year', 'yield', 'zealand', 'zen', 'zero', 'zeta']\n",
      "(52, 1251)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(articles_cleaned)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_names = []\n",
    "file = open(fileName + '_nodes.txt', \"r\")\n",
    "for line in file:\n",
    "    article_names.append(line.rstrip(\"\\n\"))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine_learning',\n",
       " 'Applied_machine_learning',\n",
       " 'AlphaGo',\n",
       " 'Artificial_neural_networks',\n",
       " 'Deep_learning',\n",
       " 'Neural_network_software',\n",
       " 'Bayesian_networks',\n",
       " 'Classification_algorithms',\n",
       " 'Decision_trees',\n",
       " 'Ensemble_learning',\n",
       " 'Cluster_analysis',\n",
       " 'Cluster_analysis_algorithms',\n",
       " 'Clustering_criteria',\n",
       " 'Computational_learning_theory',\n",
       " 'Artificial_intelligence_conferences',\n",
       " 'Signal_processing_conferences',\n",
       " 'Data_mining_and_machine_learning_software',\n",
       " 'Social_network_analysis_software',\n",
       " 'Datasets_in_machine_learning',\n",
       " 'Datasets_in_computer_vision',\n",
       " 'Dimension_reduction',\n",
       " 'Factor_analysis',\n",
       " 'Evolutionary_algorithms',\n",
       " 'Gene_expression_programming',\n",
       " 'Genetic_algorithms',\n",
       " 'Artificial_immune_systems',\n",
       " 'Genetic_programming',\n",
       " 'Nature-inspired_metaheuristics',\n",
       " 'Inductive_logic_programming',\n",
       " 'Kernel_methods_for_machine_learning',\n",
       " 'Support_vector_machines',\n",
       " 'Latent_variable_models',\n",
       " 'Structural_equation_models',\n",
       " 'Learning_in_computer_vision',\n",
       " 'Log-linear_models',\n",
       " 'Loss_functions',\n",
       " 'Machine_learning_algorithms',\n",
       " 'Machine_learning_task',\n",
       " 'Markov_models',\n",
       " 'Hidden_Markov_models',\n",
       " 'Markov_networks',\n",
       " 'Ontology_learning_(computer_science)',\n",
       " 'Reinforcement_learning',\n",
       " 'Machine_learning_researchers',\n",
       " 'Semisupervised_learning',\n",
       " 'Statistical_natural_language_processing',\n",
       " 'Language_modeling',\n",
       " 'Structured_prediction',\n",
       " 'Graphical_models',\n",
       " 'Causal_inference',\n",
       " 'Supervised_learning',\n",
       " 'Unsupervised_learning']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1121x7034 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 86605 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"article_name\"] = article_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to a file\n",
    "df.to_hdf(fileName + \"_embeddings.h5\", key='df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read from file\n",
    "data_table = pd.read_hdf(\"data/embeddings.h5\", 'df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([316, 842, 887], dtype='int64')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table[\"article_name\"]\n",
    "data_table.index[data_table['article_name'] == \"Julius_Caesar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = pd.read_hdf(fileName + \"_embeddings.h5\", 'df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaai</th>\n",
       "      <th>ab</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abstract</th>\n",
       "      <th>access</th>\n",
       "      <th>account</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>achieve</th>\n",
       "      <th>acm</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>write</th>\n",
       "      <th>year</th>\n",
       "      <th>yield</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zen</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeta</th>\n",
       "      <th>article_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Machine_learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Applied_machine_learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028689</td>\n",
       "      <td>0.054090</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>AlphaGo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Artificial_neural_networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Deep_learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Neural_network_software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Bayesian_networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Classification_algorithms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Decision_trees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057488</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Ensemble_learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Cluster_analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Cluster_analysis_algorithms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Clustering_criteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.275327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Computational_learning_theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.752925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Artificial_intelligence_conferences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Signal_processing_conferences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Data_mining_and_machine_learning_software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Social_network_analysis_software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Datasets_in_machine_learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Datasets_in_computer_vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Dimension_reduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026082</td>\n",
       "      <td>0.016693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055891</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Factor_analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026082</td>\n",
       "      <td>0.016693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055891</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Evolutionary_algorithms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027823</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Gene_expression_programming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051410</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Genetic_algorithms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062535</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Artificial_immune_systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Genetic_programming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Nature-inspired_metaheuristics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Inductive_logic_programming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Kernel_methods_for_machine_learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030538</td>\n",
       "      <td>Support_vector_machines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030538</td>\n",
       "      <td>Latent_variable_models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030538</td>\n",
       "      <td>Structural_equation_models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Learning_in_computer_vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Log-linear_models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Loss_functions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Machine_learning_algorithms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Machine_learning_task</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Markov_models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Hidden_Markov_models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Markov_networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Ontology_learning_(computer_science)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Reinforcement_learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Machine_learning_researchers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Semisupervised_learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048751</td>\n",
       "      <td>0.105451</td>\n",
       "      <td>0.111863</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Statistical_natural_language_processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Language_modeling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Structured_prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Graphical_models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Causal_inference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Supervised_learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Unsupervised_learning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52 rows × 1252 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aaai        ab   ability      able  abstract    access   account  \\\n",
       "0   0.000000  0.000000  0.024027  0.000000  0.000000  0.000000  0.000000   \n",
       "1   0.000000  0.000000  0.024027  0.000000  0.000000  0.000000  0.000000   \n",
       "2   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4   0.000000  0.000000  0.021651  0.000000  0.000000  0.000000  0.000000   \n",
       "5   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "9   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "11  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "12  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "13  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "14  0.752925  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "15  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "16  0.000000  0.000000  0.000000  0.000000  0.000000  0.178009  0.000000   \n",
       "17  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "18  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "19  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "20  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "21  0.000000  0.026082  0.016693  0.000000  0.000000  0.000000  0.055891   \n",
       "22  0.000000  0.026082  0.016693  0.000000  0.000000  0.000000  0.055891   \n",
       "23  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "24  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "25  0.000000  0.000000  0.000000  0.000000  0.086095  0.000000  0.000000   \n",
       "26  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "27  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "28  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "29  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "30  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "31  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "32  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "33  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "34  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "35  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "36  0.000000  0.000000  0.024027  0.000000  0.000000  0.000000  0.000000   \n",
       "37  0.000000  0.000000  0.024027  0.000000  0.000000  0.000000  0.000000   \n",
       "38  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "39  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "40  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "41  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "42  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "43  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "44  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "45  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "46  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "47  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "48  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "49  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "50  0.000000  0.000000  0.000000  0.058174  0.000000  0.000000  0.000000   \n",
       "51  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "    accuracy   achieve       acm                    ...                      \\\n",
       "0   0.043990  0.000000  0.000000                    ...                       \n",
       "1   0.043990  0.000000  0.000000                    ...                       \n",
       "2   0.000000  0.032462  0.000000                    ...                       \n",
       "3   0.030781  0.000000  0.000000                    ...                       \n",
       "4   0.000000  0.000000  0.000000                    ...                       \n",
       "5   0.000000  0.000000  0.000000                    ...                       \n",
       "6   0.000000  0.000000  0.000000                    ...                       \n",
       "7   0.000000  0.000000  0.000000                    ...                       \n",
       "8   0.000000  0.000000  0.000000                    ...                       \n",
       "9   0.057488  0.000000  0.000000                    ...                       \n",
       "10  0.000000  0.000000  0.000000                    ...                       \n",
       "11  0.000000  0.000000  0.000000                    ...                       \n",
       "12  0.000000  0.000000  0.000000                    ...                       \n",
       "13  0.000000  0.000000  0.275327                    ...                       \n",
       "14  0.000000  0.000000  0.000000                    ...                       \n",
       "15  0.000000  0.000000  0.000000                    ...                       \n",
       "16  0.000000  0.000000  0.000000                    ...                       \n",
       "17  0.000000  0.000000  0.000000                    ...                       \n",
       "18  0.000000  0.000000  0.000000                    ...                       \n",
       "19  0.000000  0.000000  0.000000                    ...                       \n",
       "20  0.000000  0.000000  0.000000                    ...                       \n",
       "21  0.000000  0.000000  0.000000                    ...                       \n",
       "22  0.000000  0.000000  0.000000                    ...                       \n",
       "23  0.000000  0.000000  0.000000                    ...                       \n",
       "24  0.000000  0.000000  0.000000                    ...                       \n",
       "25  0.000000  0.000000  0.000000                    ...                       \n",
       "26  0.000000  0.000000  0.000000                    ...                       \n",
       "27  0.000000  0.000000  0.000000                    ...                       \n",
       "28  0.000000  0.000000  0.000000                    ...                       \n",
       "29  0.000000  0.000000  0.000000                    ...                       \n",
       "30  0.000000  0.000000  0.000000                    ...                       \n",
       "31  0.000000  0.000000  0.000000                    ...                       \n",
       "32  0.000000  0.000000  0.000000                    ...                       \n",
       "33  0.000000  0.000000  0.000000                    ...                       \n",
       "34  0.000000  0.000000  0.000000                    ...                       \n",
       "35  0.000000  0.000000  0.000000                    ...                       \n",
       "36  0.043990  0.000000  0.000000                    ...                       \n",
       "37  0.043990  0.000000  0.000000                    ...                       \n",
       "38  0.000000  0.000000  0.000000                    ...                       \n",
       "39  0.000000  0.000000  0.000000                    ...                       \n",
       "40  0.000000  0.000000  0.000000                    ...                       \n",
       "41  0.000000  0.000000  0.000000                    ...                       \n",
       "42  0.000000  0.034189  0.000000                    ...                       \n",
       "43  0.000000  0.000000  0.000000                    ...                       \n",
       "44  0.000000  0.000000  0.000000                    ...                       \n",
       "45  0.000000  0.000000  0.000000                    ...                       \n",
       "46  0.000000  0.000000  0.000000                    ...                       \n",
       "47  0.000000  0.000000  0.000000                    ...                       \n",
       "48  0.000000  0.000000  0.000000                    ...                       \n",
       "49  0.000000  0.000000  0.000000                    ...                       \n",
       "50  0.000000  0.000000  0.000000                    ...                       \n",
       "51  0.000000  0.000000  0.000000                    ...                       \n",
       "\n",
       "        work     world     write      year     yield   zealand       zen  \\\n",
       "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2   0.000000  0.078130  0.000000  0.032462  0.000000  0.000000  0.028689   \n",
       "3   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4   0.000000  0.000000  0.000000  0.023197  0.000000  0.000000  0.000000   \n",
       "5   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6   0.022313  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7   0.094730  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "9   0.000000  0.000000  0.000000  0.000000  0.036505  0.000000  0.000000   \n",
       "10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "11  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "12  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "13  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "14  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "15  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "16  0.000000  0.000000  0.000000  0.000000  0.000000  0.101719  0.000000   \n",
       "17  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "18  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "19  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "20  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "21  0.000000  0.000000  0.000000  0.000000  0.022641  0.000000  0.000000   \n",
       "22  0.000000  0.000000  0.000000  0.000000  0.022641  0.000000  0.000000   \n",
       "23  0.027823  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "24  0.051410  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "25  0.062535  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "26  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "27  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "28  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "29  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "30  0.017648  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "31  0.017648  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "32  0.017648  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "33  0.000000  0.029315  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "34  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "35  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "36  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "37  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "38  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "39  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "40  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "41  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "42  0.038408  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "43  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "44  0.000000  0.000000  0.000000  0.000000  0.057795  0.000000  0.000000   \n",
       "45  0.048751  0.105451  0.111863  0.000000  0.000000  0.000000  0.000000   \n",
       "46  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "47  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "48  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "49  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "50  0.033804  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "51  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        zero      zeta                               article_name  \n",
       "0   0.000000  0.000000                           Machine_learning  \n",
       "1   0.000000  0.000000                   Applied_machine_learning  \n",
       "2   0.054090  0.000000                                    AlphaGo  \n",
       "3   0.000000  0.000000                 Artificial_neural_networks  \n",
       "4   0.000000  0.000000                              Deep_learning  \n",
       "5   0.000000  0.000000                    Neural_network_software  \n",
       "6   0.000000  0.000000                          Bayesian_networks  \n",
       "7   0.000000  0.000000                  Classification_algorithms  \n",
       "8   0.000000  0.000000                             Decision_trees  \n",
       "9   0.000000  0.000000                          Ensemble_learning  \n",
       "10  0.000000  0.000000                           Cluster_analysis  \n",
       "11  0.000000  0.000000                Cluster_analysis_algorithms  \n",
       "12  0.000000  0.000000                        Clustering_criteria  \n",
       "13  0.000000  0.000000              Computational_learning_theory  \n",
       "14  0.000000  0.000000        Artificial_intelligence_conferences  \n",
       "15  0.000000  0.000000              Signal_processing_conferences  \n",
       "16  0.000000  0.000000  Data_mining_and_machine_learning_software  \n",
       "17  0.000000  0.000000           Social_network_analysis_software  \n",
       "18  0.000000  0.000000               Datasets_in_machine_learning  \n",
       "19  0.000000  0.000000                Datasets_in_computer_vision  \n",
       "20  0.000000  0.000000                        Dimension_reduction  \n",
       "21  0.013797  0.000000                            Factor_analysis  \n",
       "22  0.013797  0.000000                    Evolutionary_algorithms  \n",
       "23  0.000000  0.000000                Gene_expression_programming  \n",
       "24  0.000000  0.000000                         Genetic_algorithms  \n",
       "25  0.000000  0.000000                  Artificial_immune_systems  \n",
       "26  0.000000  0.000000                        Genetic_programming  \n",
       "27  0.000000  0.000000             Nature-inspired_metaheuristics  \n",
       "28  0.000000  0.000000                Inductive_logic_programming  \n",
       "29  0.000000  0.000000        Kernel_methods_for_machine_learning  \n",
       "30  0.000000  0.030538                    Support_vector_machines  \n",
       "31  0.000000  0.030538                     Latent_variable_models  \n",
       "32  0.000000  0.030538                 Structural_equation_models  \n",
       "33  0.000000  0.000000                Learning_in_computer_vision  \n",
       "34  0.000000  0.000000                          Log-linear_models  \n",
       "35  0.000000  0.000000                             Loss_functions  \n",
       "36  0.000000  0.000000                Machine_learning_algorithms  \n",
       "37  0.000000  0.000000                      Machine_learning_task  \n",
       "38  0.000000  0.000000                              Markov_models  \n",
       "39  0.000000  0.000000                       Hidden_Markov_models  \n",
       "40  0.000000  0.000000                            Markov_networks  \n",
       "41  0.000000  0.000000       Ontology_learning_(computer_science)  \n",
       "42  0.000000  0.000000                     Reinforcement_learning  \n",
       "43  0.000000  0.000000               Machine_learning_researchers  \n",
       "44  0.000000  0.000000                    Semisupervised_learning  \n",
       "45  0.000000  0.000000    Statistical_natural_language_processing  \n",
       "46  0.000000  0.000000                          Language_modeling  \n",
       "47  0.000000  0.000000                      Structured_prediction  \n",
       "48  0.000000  0.000000                           Graphical_models  \n",
       "49  0.000000  0.000000                           Causal_inference  \n",
       "50  0.000000  0.000000                        Supervised_learning  \n",
       "51  0.000000  0.000000                      Unsupervised_learning  \n",
       "\n",
       "[52 rows x 1252 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial_neural_networks'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table.loc[1, \"article_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              Machine_learning\n",
       "1                      Applied_machine_learning\n",
       "2                                       AlphaGo\n",
       "3                    Artificial_neural_networks\n",
       "4                                 Deep_learning\n",
       "5                       Neural_network_software\n",
       "6                             Bayesian_networks\n",
       "7                     Classification_algorithms\n",
       "8                                Decision_trees\n",
       "9                             Ensemble_learning\n",
       "10                             Cluster_analysis\n",
       "11                  Cluster_analysis_algorithms\n",
       "12                          Clustering_criteria\n",
       "13                Computational_learning_theory\n",
       "14          Artificial_intelligence_conferences\n",
       "15                Signal_processing_conferences\n",
       "16    Data_mining_and_machine_learning_software\n",
       "17             Social_network_analysis_software\n",
       "18                 Datasets_in_machine_learning\n",
       "19                  Datasets_in_computer_vision\n",
       "20                          Dimension_reduction\n",
       "21                              Factor_analysis\n",
       "22                      Evolutionary_algorithms\n",
       "23                  Gene_expression_programming\n",
       "24                           Genetic_algorithms\n",
       "25                    Artificial_immune_systems\n",
       "26                          Genetic_programming\n",
       "27               Nature-inspired_metaheuristics\n",
       "28                  Inductive_logic_programming\n",
       "29          Kernel_methods_for_machine_learning\n",
       "30                      Support_vector_machines\n",
       "31                       Latent_variable_models\n",
       "32                   Structural_equation_models\n",
       "33                  Learning_in_computer_vision\n",
       "34                            Log-linear_models\n",
       "35                               Loss_functions\n",
       "36                  Machine_learning_algorithms\n",
       "37                        Machine_learning_task\n",
       "38                                Markov_models\n",
       "39                         Hidden_Markov_models\n",
       "40                              Markov_networks\n",
       "41         Ontology_learning_(computer_science)\n",
       "42                       Reinforcement_learning\n",
       "43                 Machine_learning_researchers\n",
       "44                      Semisupervised_learning\n",
       "45      Statistical_natural_language_processing\n",
       "46                            Language_modeling\n",
       "47                        Structured_prediction\n",
       "48                             Graphical_models\n",
       "49                             Causal_inference\n",
       "50                          Supervised_learning\n",
       "51                        Unsupervised_learning\n",
       "Name: article_name, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table[\"article_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial_neural_networks Machine_learning\n",
      "Applied_machine_learning Machine_learning\n",
      "Unsupervised_learning Machine_learning\n",
      "Structured_prediction Machine_learning\n",
      "Loss_functions Machine_learning\n",
      "Support_vector_machines Machine_learning\n",
      "Latent_variable_models Machine_learning\n",
      "Cluster_analysis Machine_learning\n",
      "Signal_processing_conferences Machine_learning\n",
      "Artificial_intelligence_conferences Machine_learning\n",
      "Data_mining_and_machine_learning_software Machine_learning\n",
      "Machine_learning_algorithms Machine_learning\n",
      "Bayesian_networks Machine_learning\n",
      "Machine_learning_portal Machine_learning\n",
      "Statistical_natural_language_processing Machine_learning\n",
      "Markov_models Machine_learning\n",
      "Evolutionary_algorithms Machine_learning\n",
      "Genetic_programming Machine_learning\n",
      "Datasets_in_machine_learning Machine_learning\n",
      "Dimension_reduction Machine_learning\n",
      "Machine_learning_researchers Machine_learning\n",
      "Supervised_learning Machine_learning\n",
      "Semisupervised_learning Machine_learning\n",
      "Computational_learning_theory Machine_learning\n",
      "Kernel_methods_for_machine_learning Machine_learning\n",
      "Learning_in_computer_vision Machine_learning\n",
      "Inductive_logic_programming Machine_learning\n",
      "Log-linear_models Machine_learning\n",
      "Classification_algorithms Machine_learning\n",
      "Machine_learning_task Machine_learning\n",
      "Ensemble_learning Machine_learning\n",
      "Neural_network_software Artificial_neural_networks\n",
      "Deep_learning Artificial_neural_networks\n",
      "AlphaGo Applied_machine_learning\n",
      "Graphical_models Structured_prediction\n",
      "Bayesian_networks Graphical_models\n",
      "Causal_inference Graphical_models\n",
      "Markov_networks Graphical_models\n",
      "Structural_equation_models Latent_variable_models\n",
      "Factor_analysis Latent_variable_models\n",
      "Equivalence_classes Cluster_analysis\n",
      "Clustering_criteria Cluster_analysis\n",
      "Cluster_analysis_algorithms Cluster_analysis\n",
      "Social_network_analysis_software Data_mining_and_machine_learning_software\n",
      "Genetic_algorithms Machine_learning_algorithms\n",
      "Artificial_immune_systems Genetic_algorithms\n",
      "Gene_expression_programming Genetic_algorithms\n",
      "Language_modeling Statistical_natural_language_processing\n",
      "Hidden_Markov_models Markov_models\n",
      "Markov_networks Markov_models\n",
      "Genetic_programming Evolutionary_algorithms\n",
      "Genetic_algorithms Evolutionary_algorithms\n",
      "Gene_expression_programming Evolutionary_algorithms\n",
      "Nature-inspired_metaheuristics Evolutionary_algorithms\n",
      "Artificial_immune_systems Genetic_algorithms\n",
      "Gene_expression_programming Genetic_algorithms\n",
      "Datasets_in_computer_vision Datasets_in_machine_learning\n",
      "Factor_analysis Dimension_reduction\n",
      "Support_vector_machines Kernel_methods_for_machine_learning\n",
      "Ensemble_learning Classification_algorithms\n",
      "Artificial_neural_networks Classification_algorithms\n",
      "Decision_trees Classification_algorithms\n",
      "Neural_network_software Artificial_neural_networks\n",
      "Deep_learning Artificial_neural_networks\n"
     ]
    }
   ],
   "source": [
    "file = open(fileName + \"_relations.txt\", \"r\")\n",
    "for line in file:\n",
    "    print(line.rstrip(\"\\n\"))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
