Convex analysis is the branch of mathematics devoted to the study of properties of convex functions and convex sets, often with applications in convex minimization, a subdomain of optimization theory.


== Convex sets ==

A convex set is a set C ⊆ X, for some vector space X, such that for any x, y ∈ C and λ ∈ [0, 1] then

  
    
      
        λ
        x
        +
        (
        1
        −
        λ
        )
        y
        ∈
        C
      
    
    {\displaystyle \lambda x+(1-\lambda )y\in C}
  .


== Convex functions ==

A convex function is any extended real-valued function f : X → R ∪ {±∞} which satisfies Jensen's inequality, i.e. for any x, y ∈ X and any λ ∈ [0, 1] then

  
    
      
        f
        (
        λ
        x
        +
        (
        1
        −
        λ
        )
        y
        )
        ≤
        λ
        f
        (
        x
        )
        +
        (
        1
        −
        λ
        )
        f
        (
        y
        )
      
    
    {\displaystyle f(\lambda x+(1-\lambda )y)\leq \lambda f(x)+(1-\lambda )f(y)}
  .Equivalently, a convex function is any (extended) real valued function such that its epigraph

  
    
      
        
          {
          
            (
            x
            ,
            r
            )
            ∈
            X
            ×
            
              R
            
            :
            f
            (
            x
            )
            ≤
            r
          
          }
        
      
    
    {\displaystyle \left\{(x,r)\in X\times \mathbf {R} :f(x)\leq r\right\}}
  is a convex set.


== Convex conjugate ==

The convex conjugate of an extended real-valued (not necessarily convex) function f : X → R ∪ {±∞} is f* : X* → R ∪ {±∞} where X* is the dual space of X, and

  
    
      
        
          f
          
            ∗
          
        
        (
        
          x
          
            ∗
          
        
        )
        =
        
          sup
          
            x
            ∈
            X
          
        
        
          {
          
            ⟨
            
              x
              
                ∗
              
            
            ,
            x
            ⟩
            −
            f
            (
            x
            )
          
          }
        
        .
      
    
    {\displaystyle f^{*}(x^{*})=\sup _{x\in X}\left\{\langle x^{*},x\rangle -f(x)\right\}.}
  


=== Biconjugate ===
The biconjugate of a function f : X → R ∪ {±∞} is the conjugate of the conjugate, typically written as f** : X → R ∪ {±∞}. The biconjugate is useful for showing when strong or weak duality hold (via the perturbation function).
For any x ∈ X the inequality f**(x) ≤ f(x) follows from the Fenchel–Young inequality.  For proper functions, f = f** if and only if f is convex and lower semi-continuous by Fenchel–Moreau theorem.


== Convex minimization ==

A convex minimization (primal) problem is one of the form

  
    
      
        
          inf
          
            x
            ∈
            M
          
        
        f
        (
        x
        )
      
    
    {\displaystyle \inf _{x\in M}f(x)}
  such that f : X → R ∪ {±∞} is a convex function and M ⊆ X is a convex set.


=== Dual problem ===

In optimization theory, the duality principle states that optimization problems may be viewed from either of two perspectives, the primal problem or the dual problem.
In general given two dual pairs separated locally convex spaces (X, X*) and (Y, Y*). Then given the function f : X → R ∪ {+∞}, we can define the primal problem as finding x such that

  
    
      
        
          inf
          
            x
            ∈
            X
          
        
        f
        (
        x
        )
        .
      
    
    {\displaystyle \inf _{x\in X}f(x).}
  If there are constraint conditions, these can be built into the function f by letting 
  
    
      
        f
        =
        f
        +
        
          I
          
            
              c
              o
              n
              s
              t
              r
              a
              i
              n
              t
              s
            
          
        
      
    
    {\displaystyle f=f+I_{\mathrm {constraints} }}
   where I is the indicator function.  Then let F : X × Y → R ∪ {±∞} be a perturbation function such that F(x, 0) = f(x).The dual problem with respect to the chosen perturbation function is given by

  
    
      
        
          sup
          
            
              y
              
                ∗
              
            
            ∈
            
              Y
              
                ∗
              
            
          
        
        −
        
          F
          
            ∗
          
        
        (
        0
        ,
        
          y
          
            ∗
          
        
        )
      
    
    {\displaystyle \sup _{y^{*}\in Y^{*}}-F^{*}(0,y^{*})}
  where F* is the convex conjugate in both variables of F.
The duality gap is the difference of the right and left hand sides of the inequality

  
    
      
        
          sup
          
            
              y
              
                ∗
              
            
            ∈
            
              Y
              
                ∗
              
            
          
        
        −
        
          F
          
            ∗
          
        
        (
        0
        ,
        
          y
          
            ∗
          
        
        )
        ≤
        
          inf
          
            x
            ∈
            X
          
        
        F
        (
        x
        ,
        0
        )
        .
      
    
    {\displaystyle \sup _{y^{*}\in Y^{*}}-F^{*}(0,y^{*})\leq \inf _{x\in X}F(x,0).}
  This principle is the same as weak duality.  If the two sides are equal to each other, then the problem is said to satisfy strong duality.
There are many conditions for strong duality to hold such as:

F = F** where F is the perturbation function relating the primal and dual problems and F** is the biconjugate of F;
the primal problem is a linear optimization problem;
Slater's condition for a convex optimization problem.


==== Lagrange duality ====
For a convex minimization problem with inequality constraints,

minx f(x) subject to gi(x) ≤ 0 for i = 1, ..., m.the Lagrangian dual problem is

supu infx L(x, u) subject to ui(x) ≥ 0 for i = 1, ..., m.where the objective function L(x, u) is the Lagrange dual function defined as follows:

  
    
      
        L
        (
        x
        ,
        u
        )
        =
        f
        (
        x
        )
        +
        
          ∑
          
            j
            =
            1
          
          
            m
          
        
        
          u
          
            j
          
        
        
          g
          
            j
          
        
        (
        x
        )
      
    
    {\displaystyle L(x,u)=f(x)+\sum _{j=1}^{m}u_{j}g_{j}(x)}
  


== See also ==
List of convexity topics
Werner Fenchel


== Notes ==


== References ==
J.-B. Hiriart-Urruty; C. Lemaréchal (2001). Fundamentals of convex analysis. Berlin: Springer-Verlag. ISBN 978-3-540-42205-1.
Singer, Ivan (1997). Abstract convex analysis. Canadian Mathematical Society series of monographs and advanced texts. New York: John Wiley & Sons, Inc. pp. xxii+491. ISBN 0-471-16015-6. MR 1461544.
Stoer, J.; Witzgall, C. (1970). Convexity and optimization in finite dimensions. 1. Berlin: Springer. ISBN 978-0-387-04835-2.
A.G. Kusraev; S.S. Kutateladze (1995). Subdifferentials: Theory and Applications. Dordrecht: Kluwer Academic Publishers. ISBN 978-94-011-0265-0.


== External links ==
 Media related to Convex analysis at Wikimedia Commons