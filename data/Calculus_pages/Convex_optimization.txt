Convex optimization is a subfield of mathematical optimization that studies the problem of minimizing convex functions over convex sets. Many classes of convex optimization problems admit polynomial-time algorithms, whereas mathematical optimization is in general NP-hard.Convex optimization has applications in a wide range of disciplines, such as automatic control systems, estimation and signal processing, communications and networks, electronic circuit design, data analysis and modeling, finance, statistics (optimal experimental design), and structural optimization, where the approximation concept has proven to be efficient.  
With recent advancements in computing and optimization algorithms, convex programming is nearly as straightforward as linear programming.


== Definition ==
A convex optimization problem is an optimization problem in which the objective function is a convex function and the feasible set is a convex set. A function 
  
    
      
        f
      
    
    {\displaystyle f}
    mapping some subset of 
  
    
      
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle \mathbb {R} ^{n}}
  into 
  
    
      
        
          R
        
        ∪
        {
        ±
        ∞
        }
      
    
    {\displaystyle \mathbb {R} \cup \{\pm \infty \}}
   is convex if its domain is convex and for all 
  
    
      
        θ
        ∈
        [
        0
        ,
        1
        ]
      
    
    {\displaystyle \theta \in [0,1]}
   and all 
  
    
      
        x
        ,
        y
      
    
    {\displaystyle x,y}
   in its domain, the following condition holds:
  
    
      
        f
        (
        θ
        x
        +
        (
        1
        −
        θ
        )
        y
        )
        ≤
        θ
        f
        (
        x
        )
        +
        (
        1
        −
        θ
        )
        f
        (
        y
        )
      
    
    {\displaystyle f(\theta x+(1-\theta )y)\leq \theta f(x)+(1-\theta )f(y)}
  . A set S is convex if for all members 
  
    
      
        x
        ,
        y
        ∈
        S
      
    
    {\displaystyle x,y\in S}
   and all  
  
    
      
        θ
        ∈
        [
        0
        ,
        1
        ]
      
    
    {\displaystyle \theta \in [0,1]}
  , we have that 
  
    
      
        θ
        x
        +
        (
        1
        −
        θ
        )
        y
        ∈
        S
      
    
    {\displaystyle \theta x+(1-\theta )y\in S}
  .
Concretely, a convex optimization problem is the problem of finding some 
  
    
      
        
          
            x
            
              ∗
            
          
        
        ∈
        C
      
    
    {\displaystyle \mathbf {x^{\ast }} \in C}
   attaining

  
    
      
        inf
        {
        f
        (
        
          x
        
        )
        :
        
          x
        
        ∈
        C
        }
      
    
    {\displaystyle \inf\{f(\mathbf {x} ):\mathbf {x} \in C\}}
  ,where the objective function 
  
    
      
        f
      
    
    {\displaystyle f}
   is convex, as is the feasible set 
  
    
      
        C
      
    
    {\displaystyle C}
  .  If such a point exists, it is referred to as an optimal point; the set of all optimal points is called the optimal set. If 
  
    
      
        f
      
    
    {\displaystyle f}
   is unbounded below over 
  
    
      
        C
      
    
    {\displaystyle C}
   or the infimum is not attained, then the optimization problem is said to be unbounded. Otherwise, if 
  
    
      
        C
      
    
    {\displaystyle C}
   is the empty set, then the problem is said to be infeasible.


=== Standard form ===
A convex optimization problem is said to be in the standard form if it is written as

  
    
      
        
          
            
              
              
                
                  
                    minimize
                    
                      x
                    
                  
                
              
              
              
                f
                (
                
                  x
                
                )
              
            
            
              
              
                
                  s
                  u
                  b
                  j
                  e
                  c
                  t
                   
                  t
                  o
                
              
              
              
                
                  g
                  
                    i
                  
                
                (
                
                  x
                
                )
                ≤
                0
                ,
                
                i
                =
                1
                ,
                …
                ,
                m
              
            
            
              
              
              
              
                
                  h
                  
                    i
                  
                
                (
                
                  x
                
                )
                =
                0
                ,
                
                i
                =
                1
                ,
                …
                ,
                p
                ,
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&{\underset {\mathbf {x} }{\operatorname {minimize} }}&&f(\mathbf {x} )\\&\operatorname {subject\ to} &&g_{i}(\mathbf {x} )\leq 0,\quad i=1,\dots ,m\\&&&h_{i}(\mathbf {x} )=0,\quad i=1,\dots ,p,\end{aligned}}}
  where 
  
    
      
        x
        ∈
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle x\in \mathbb {R} ^{n}}
   is the optimization variable, the functions 
  
    
      
        f
        ,
        
          g
          
            1
          
        
        ,
        …
        ,
        
          g
          
            m
          
        
      
    
    {\displaystyle f,g_{1},\ldots ,g_{m}}
   are convex, and the functions 
  
    
      
        
          h
          
            1
          
        
        ,
        …
        ,
        
          h
          
            p
          
        
      
    
    {\displaystyle h_{1},\ldots ,h_{p}}
   are affine. 
In this notation, the function 
  
    
      
        f
      
    
    {\displaystyle f}
   is the objective function of the problem, and the functions 
  
    
      
        
          g
          
            i
          
        
      
    
    {\displaystyle g_{i}}
   and 
  
    
      
        
          h
          
            i
          
        
      
    
    {\displaystyle h_{i}}
  are referred to as the constraint functions. The feasible set of the optimization problem is the set consisting of all points 
  
    
      
        x
        ∈
        
          
            R
          
          
            n
          
        
      
    
    {\displaystyle x\in \mathbb {R} ^{n}}
  satisfying 
  
    
      
        
          g
          
            1
          
        
        (
        x
        )
        ≤
        0
        ,
        …
        ,
        
          g
          
            m
          
        
        (
        x
        )
        ≤
        0
      
    
    {\displaystyle g_{1}(x)\leq 0,\ldots ,g_{m}(x)\leq 0}
   and 
  
    
      
        
          h
          
            1
          
        
        (
        x
        )
        =
        0
        ,
        …
        ,
        
          h
          
            p
          
        
        (
        x
        )
        =
        0
      
    
    {\displaystyle h_{1}(x)=0,\ldots ,h_{p}(x)=0}
  . This set is convex because the sublevel sets of convex functions are convex, affine sets are convex, and the intersection of convex sets is convex. Many optimization problems can be equivalently formulated in this standard form. For example, the problem of maximizing a concave function 
  
    
      
        f
      
    
    {\displaystyle f}
   can be re-formulated equivalently as the problem of minimizing the convex function 
  
    
      
        −
        f
      
    
    {\displaystyle -f}
  ; as such, the problem of maximizing a concave function over a convex set is often referred to as a convex optimization problem.


== Properties ==
The following are useful properties of convex optimization problems:
every local minimum is a global minimum;
the optimal set is convex;
if the objective function is strictly convex, then the problem has at most one optimal point.These results are used by the theory of convex minimization along with geometric notions from functional analysis (in Hilbert spaces) such as the Hilbert projection theorem, the separating hyperplane theorem, and Farkas' lemma.


== Examples ==
The following problem classes are all convex optimization problems, or can be reduced to convex optimization problems via simple transformations:  

Least squares
Linear programming
Convex quadratic minimization with linear constraints
Quadratic minimization with convex quadratic constraints
Conic optimization
Geometric programming
Second order cone programming
Semidefinite programming
Entropy maximization with appropriate constraints


== Lagrange multipliers ==
Consider a convex minimization problem given in standard form by a cost function 
  
    
      
        f
        (
        x
        )
      
    
    {\displaystyle f(x)}
   and inequality constraints 
  
    
      
        
          g
          
            i
          
        
        (
        x
        )
        ≤
        0
      
    
    {\displaystyle g_{i}(x)\leq 0}
   for 
  
    
      
        1
        ≤
        i
        ≤
        m
      
    
    {\displaystyle 1\leq i\leq m}
  . Then the domain 
  
    
      
        
          
            X
          
        
      
    
    {\displaystyle {\mathcal {X}}}
   is:

  
    
      
        
          
            X
          
        
        =
        
          {
          
            x
            ∈
            X
            |
            
              g
              
                1
              
            
            (
            x
            )
            ,
            …
            ,
            
              g
              
                m
              
            
            (
            x
            )
            ≤
            0
          
          }
        
        .
      
    
    {\displaystyle {\mathcal {X}}=\left\{x\in X\vert g_{1}(x),\ldots ,g_{m}(x)\leq 0\right\}.}
  The Lagrangian function for the problem is

  
    
      
        L
        (
        x
        ,
        
          λ
          
            0
          
        
        ,
        
          λ
          
            1
          
        
        ,
        …
        ,
        
          λ
          
            m
          
        
        )
        =
        
          λ
          
            0
          
        
        f
        (
        x
        )
        +
        
          λ
          
            1
          
        
        
          g
          
            1
          
        
        (
        x
        )
        +
        ⋯
        +
        
          λ
          
            m
          
        
        
          g
          
            m
          
        
        (
        x
        )
        .
      
    
    {\displaystyle L(x,\lambda _{0},\lambda _{1},\ldots ,\lambda _{m})=\lambda _{0}f(x)+\lambda _{1}g_{1}(x)+\cdots +\lambda _{m}g_{m}(x).}
  For each point 
  
    
      
        x
      
    
    {\displaystyle x}
   in 
  
    
      
        X
      
    
    {\displaystyle X}
   that minimizes 
  
    
      
        f
      
    
    {\displaystyle f}
   over 
  
    
      
        X
      
    
    {\displaystyle X}
  , there exist real numbers 
  
    
      
        
          λ
          
            0
          
        
        ,
        
          λ
          
            1
          
        
        ,
        …
        ,
        
          λ
          
            m
          
        
        ,
      
    
    {\displaystyle \lambda _{0},\lambda _{1},\ldots ,\lambda _{m},}
   called Lagrange multipliers, that satisfy these conditions simultaneously:

  
    
      
        x
      
    
    {\displaystyle x}
   minimizes 
  
    
      
        L
        (
        y
        ,
        
          λ
          
            0
          
        
        ,
        
          λ
          
            1
          
        
        ,
        …
        ,
        
          λ
          
            m
          
        
        )
      
    
    {\displaystyle L(y,\lambda _{0},\lambda _{1},\ldots ,\lambda _{m})}
   over all 
  
    
      
        y
        ∈
        X
        ,
      
    
    {\displaystyle y\in X,}
  

  
    
      
        
          λ
          
            0
          
        
        ,
        
          λ
          
            1
          
        
        ,
        …
        ,
        
          λ
          
            m
          
        
        ≥
        0
        ,
      
    
    {\displaystyle \lambda _{0},\lambda _{1},\ldots ,\lambda _{m}\geq 0,}
   with at least one 
  
    
      
        
          λ
          
            k
          
        
        >
        0
        ,
      
    
    {\displaystyle \lambda _{k}>0,}
  

  
    
      
        
          λ
          
            1
          
        
        
          g
          
            1
          
        
        (
        x
        )
        =
        ⋯
        =
        
          λ
          
            m
          
        
        
          g
          
            m
          
        
        (
        x
        )
        =
        0
      
    
    {\displaystyle \lambda _{1}g_{1}(x)=\cdots =\lambda _{m}g_{m}(x)=0}
   (complementary slackness).If there exists a "strictly feasible point", that is, a point 
  
    
      
        z
      
    
    {\displaystyle z}
   satisfying

  
    
      
        
          g
          
            1
          
        
        (
        z
        )
        ,
        …
        ,
        
          g
          
            m
          
        
        (
        z
        )
        <
        0
        ,
      
    
    {\displaystyle g_{1}(z),\ldots ,g_{m}(z)<0,}
  then the statement above can be strengthened to require that 
  
    
      
        
          λ
          
            0
          
        
        =
        1
      
    
    {\displaystyle \lambda _{0}=1}
  .
Conversely, if some 
  
    
      
        x
      
    
    {\displaystyle x}
   in 
  
    
      
        X
      
    
    {\displaystyle X}
   satisfies (1)–(3) for scalars 
  
    
      
        
          λ
          
            0
          
        
        ,
        …
        ,
        
          λ
          
            m
          
        
      
    
    {\displaystyle \lambda _{0},\ldots ,\lambda _{m}}
   with 
  
    
      
        
          λ
          
            0
          
        
        =
        1
      
    
    {\displaystyle \lambda _{0}=1}
   then 
  
    
      
        x
      
    
    {\displaystyle x}
   is certain to minimize 
  
    
      
        f
      
    
    {\displaystyle f}
   over 
  
    
      
        X
      
    
    {\displaystyle X}
  .


== Algorithms ==
Convex optimization problems can be solved by the following contemporary methods:
Bundle methods (Wolfe, Lemaréchal, Kiwiel), and
Subgradient projection methods (Polyak),
Interior-point methods, which make use of self-concordant barrier functions  and self-regular barrier functions.
Cutting-plane methods
Ellipsoid method
Subgradient method
Dual subgradients and the drift-plus-penalty methodSubgradient methods can be implemented simply and so are widely used.   Dual subgradient methods are subgradient methods applied to a dual problem.  The drift-plus-penalty method is similar to the dual subgradient method, but takes a time average of the primal variables.


== Extensions ==
Extensions of convex optimization include the optimization of biconvex, pseudo-convex, and quasiconvex functions. Extensions of the theory of convex analysis and iterative methods for approximately solving non-convex minimization problems occur in the field of generalized convexity, also known as abstract convex analysis.


== See also ==
Duality
Karush–Kuhn–Tucker conditions
Optimization problem
Proximal gradient method


== Notes ==


== References ==
Bertsekas, Dimitri P.; Nedic, Angelia; Ozdaglar, Asuman (2003). Convex Analysis and Optimization. Belmont, MA.: Athena Scientific. ISBN 978-1-886529-45-8.
Bertsekas, Dimitri P. (2009). Convex Optimization Theory. Belmont, MA.: Athena Scientific. ISBN 978-1-886529-31-1.
Bertsekas, Dimitri P. (2015). Convex Optimization Algorithms. Belmont, MA.: Athena Scientific. ISBN 978-1-886529-28-1.Boyd, Stephen P.; Vandenberghe, Lieven (2004). Convex Optimization (PDF). Cambridge University Press. ISBN 978-0-521-83378-3. Retrieved October 15, 2011.Borwein, Jonathan, and Lewis, Adrian. (2000). Convex Analysis and Nonlinear Optimization. Springer.
Christensen, Peter W.; Anders Klarbring (2008). An introduction to structural optimization. 153. Springer Science & Businees Media. ISBN 9781402086663.Hiriart-Urruty, Jean-Baptiste, and Lemaréchal, Claude. (2004). Fundamentals of Convex analysis. Berlin: Springer.
Hiriart-Urruty, Jean-Baptiste; Lemaréchal, Claude (1993). Convex analysis and minimization algorithms, Volume I: Fundamentals. Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences]. 305. Berlin: Springer-Verlag. pp. xviii+417. ISBN 978-3-540-56850-6. MR 1261420.
Hiriart-Urruty, Jean-Baptiste; Lemaréchal, Claude (1993). Convex analysis and minimization algorithms, Volume II: Advanced theory and bundle methods. Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences]. 306. Berlin: Springer-Verlag. pp. xviii+346. ISBN 978-3-540-56852-0. MR 1295240.
Kiwiel, Krzysztof C. (1985). Methods of Descent for Nondifferentiable Optimization. Lecture Notes in Mathematics. New York: Springer-Verlag. ISBN 978-3-540-15642-0.
Lemaréchal, Claude (2001). "Lagrangian relaxation".  In Michael Jünger and Denis Naddef (ed.). Computational combinatorial optimization: Papers from the Spring School held in Schloß Dagstuhl, May 15–19, 2000. Lecture Notes in Computer Science. 2241. Berlin: Springer-Verlag. pp. 112–156. doi:10.1007/3-540-45586-8_4. ISBN 978-3-540-42877-0. MR 1900016.
Nesterov, Yurii; Nemirovskii, Arkadii (1994). Interior Point Polynomial Methods in Convex Programming. SIAM.
Nesterov, Yurii. (2004). Introductory Lectures on Convex Optimization,  Kluwer Academic Publishers
Rockafellar, R. T. (1970). Convex analysis. Princeton: Princeton University Press.Ruszczyński, Andrzej (2006). Nonlinear Optimization. Princeton University Press.
Schmit, L.A.; Fleury, C. 1980: Structural synthesis by combining approximation concepts and dual methods. J. Amer. Inst. Aeronaut. Astronaut 18, 1252-1260


== External links ==
Stephen Boyd and Lieven Vandenberghe, Convex optimization  (book in pdf)
EE364a: Convex Optimization I and EE364b: Convex Optimization II, Stanford course homepages
6.253: Convex Analysis and Optimization, an MIT OCW course homepage
Brian Borchers, An overview of software for convex optimization